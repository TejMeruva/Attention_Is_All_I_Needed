**16 December 2025**
- installed `GithHub Desktop` (had been using CLI like a hobo all these months)
- found a tutorial repo
- tried writing with attention first (didn't work out)

**17 December 2025**
- finished the position encoding and embedding module
- learnt to keep track of the `tensor sizes` as was mentioned in the tutorial as a common tip.
- made a note in `Notability`
- wrote comments frequently to keep track of what's going on.
- learnt what `droupout` is
- wrote the `transformerAttention` class and learnt about attention.
- finished writing the FeedForward Network, assumed that different ffn is used for different position, turns out not so.
- Finished the `Encoder` module.
- Finished the `Decoder` module.
- Learnt what a `causal mask`  is.
- finished combining the encoder and decoder (did by mylsef)
- finished the `full transformer`!
- i am not satisfied--i need to train it right now
- i google how to train a transformer to be a chatbot
- apparently you don't even need the encoder stack.
- i will have to make a new file altogether.
- 24M parameters: crazy
- learnt why `right shifting` is needed.

**18 Decemeber 2025**
- tried the Colab TPU 
- couldn't use the local transformer module
- copy pasted the code instead
- learnt about `tokenizer` from `transformers`.
- the fuck is an `attention mask`?