{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from math import log, sqrt\n",
    "\n",
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, \n",
    "                 d_embed: int, \n",
    "                 d_model: int,\n",
    "                 dropout_p: float = 0.1,\n",
    "                 dev = 'cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.dev = dev\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_embed,\n",
    "            device=self.dev\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=d_embed,\n",
    "            out_features=d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "        self.scaling = float(sqrt(self.d_model))\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    @staticmethod # decorator that indicates that the following function doesn't operate on `self`\n",
    "    def create_positional_encoding(seq_length:int, \n",
    "                                   d_model:int, \n",
    "                                   batch_size:int,\n",
    "                                   dev = 'cpu'\n",
    "                                   ):\n",
    "\n",
    "        positions = torch.arange(seq_length, dtype=torch.long, device=dev)\\\n",
    "            .unsqueeze(1) # shape (seq_length, 1) i.e. makes it vertical\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, d_model, 2)/d_model)*(-4)*log(10)\n",
    "        ).to(dev)\n",
    "        \n",
    "        pe = torch.zeros(size=(seq_length, d_model), dtype=torch.float32, device=dev) # the tensor to be multiplied to positions tensor to get pe\n",
    "        pe[:, 0::2] = torch.sin(positions*div_term) # for even dimensions\n",
    "        pe[:, 1::2] = torch.cos(positions*div_term) # for odd dimensions\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1) # copy out the encodings for each batch\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # step 1: make embeddings\n",
    "        token_embedding = self.embedding(x)\n",
    "\n",
    "        # step 2: go from d_embed to d_model\n",
    "        token_embedding = self.projection(token_embedding) \\\n",
    "            * self.scaling # multiplying with scaling factor, just like in the paper\n",
    "\n",
    "        # step 3: add positional encoding\n",
    "        pos_encoding = self.create_positional_encoding(\n",
    "            seq_length=seq_length, \n",
    "            d_model = self.d_model,\n",
    "            batch_size=batch_size,\n",
    "            dev=self.dev\n",
    "        )\n",
    "\n",
    "        #step 4: normalize the sum of pos encoding and token_embed\n",
    "        norm_sum = self.layerNorm(pos_encoding + token_embedding)\n",
    "        op = self.dropout(norm_sum)\n",
    "        return op\n",
    "\n",
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 dropout_p: float = 0.1,\n",
    "                 dev='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if (d_model % num_heads) != 0: raise ValueError(f'`d_model` not divisible by `num_heads`')\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_heads = self.d_model // self.num_heads\n",
    "        self.scale_factor = float(1.0 / sqrt(self.d_heads))\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.dev = dev\n",
    "\n",
    "        #linear transformations\n",
    "        self.q_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "        self.v_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                seq: torch.Tensor, \n",
    "                key_value_states:torch.Tensor = None, \n",
    "                att_mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, d_model = seq.size()\n",
    "\n",
    "        Q_state: torch.Tensor = self.q_proj(seq)\n",
    "        if key_value_states is not None:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            K_state: torch.Tensor = self.k_proj(key_value_states)\n",
    "            V_state: torch.Tensor = self.v_proj(key_value_states)\n",
    "        else:\n",
    "            kv_seq_len = seq_length\n",
    "            K_state: torch.Tensor = self.k_proj(seq)\n",
    "            V_state: torch.Tensor = self.v_proj(seq)\n",
    "\n",
    "        Q_state = Q_state.view(batch_size, seq_length, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "\n",
    "        Q_state = Q_state * self.scale_factor\n",
    "        \n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask # yes, in this case the mask is not multiplied, but added. This is to ensure that after softmax the things to be excluded are 0\n",
    "        \n",
    "        att_score = F.softmax(self.att_matrix, dim=-1) # torch.nn.Softmax() is used in __init__, F.softmax() is used for these inline operations.\n",
    "        att_score = self.dropout(att_score)\n",
    "        att_op = torch.matmul(att_score, V_state)\n",
    "\n",
    "        #concatenating all heads \n",
    "        att_op = att_op.transpose(1, 2)\n",
    "        att_op = att_op.contiguous().view(batch_size, seq_length, self.num_heads*self.d_heads)\n",
    "\n",
    "        att_op = self.output_proj(att_op)\n",
    "\n",
    "        return att_op\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int,\n",
    "                 dev='cpu'):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.dev = dev\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_ff,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=self.d_ff,\n",
    "            out_features=self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.Tensor):\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "        f2 = self.fc2(f1)\n",
    "        return f2\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int, \n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1,\n",
    "                 dev='cpu'\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_ff = d_ff\n",
    "        self.dev = dev\n",
    "\n",
    "        self.att_layer = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p,\n",
    "            dev=self.dev\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff,\n",
    "            dev = self.dev\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=self.dev)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=self.dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_att = self.att_layer(x)\n",
    "\n",
    "        x_att = self.dropout(x_att)\n",
    "        x_norm1 = self.norm1(x + x_att)\n",
    "\n",
    "        x_ff = self.ffn(x_norm1)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm2 = self.norm2(x_ff + x_norm1)\n",
    "        \n",
    "        return x_norm2\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1,\n",
    "                 dev='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dev = dev\n",
    "\n",
    "        self.att_layer1 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p,\n",
    "            dev=self.dev\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=self.dev)\n",
    "\n",
    "        self.att_layer2 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p,\n",
    "            dev=self.dev\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=self.dev)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff,\n",
    "            dev=self.dev\n",
    "        )\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(self.d_model, device=self.dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len: int, dev='cpu') -> torch.Tensor:\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=dev), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, value=float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                cross_input:torch.Tensor,\n",
    "                padding_mask:torch.Tensor = None\n",
    "                ):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        causal_mask = self.create_causal_mask(seq_len=seq_length, dev=self.dev)\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1) #unsqeeze the mast for self attention\n",
    "\n",
    "        x_att1 = self.att_layer1( #self-attention\n",
    "            seq=x,\n",
    "            att_mask = causal_mask\n",
    "        )\n",
    "\n",
    "        x_att1 = self.dropout(x_att1)\n",
    "        x_norm1 = self.norm1(x_att1 + x)\n",
    "\n",
    "        x_att2 = self.att_layer2( #cross attention\n",
    "            seq=x_norm1,\n",
    "            key_value_states=cross_input,\n",
    "            att_mask = padding_mask\n",
    "        )\n",
    "\n",
    "        x_att2 = self.dropout(x_att2 + x_norm1)\n",
    "        x_norm2 = self.norm2(x_att2)\n",
    "\n",
    "        x_ff = self.ffn(x_norm2)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm3 = self.norm3(x_ff)\n",
    "        \n",
    "        return x_norm3\n",
    "\n",
    "\n",
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 N_enc: int,\n",
    "                 N_dec: int, \n",
    "                 d_model:int,\n",
    "                 num_heads: int, \n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1,\n",
    "                 dev = 'cpu'\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "        self.dev = dev\n",
    "\n",
    "        self.encoder_stack = nn.ModuleList([\n",
    "            TransformerEncoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                d_ff=self.d_ff,\n",
    "                dropout_p=self.dropout_p,\n",
    "                dev=self.dev\n",
    "            ) for _ in range(N_enc)\n",
    "        ])\n",
    "\n",
    "        self.decoder_stack = nn.ModuleList([\n",
    "            TransformerDecoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                d_ff=self.d_ff,\n",
    "                dropout_p=self.dropout_p,\n",
    "                dev = self.dev\n",
    "            ) for _ in range(N_dec)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y:torch.Tensor, padding_mask=None) -> torch.Tensor:\n",
    "        #pass through the encoder stack\n",
    "        encoder_output = x\n",
    "        for encoder in self.encoder_stack:\n",
    "            encoder_output = encoder(encoder_output)\n",
    "\n",
    "        #pass through the decoder stack\n",
    "        #uses only the final encoder input\n",
    "        decoder_output = y\n",
    "        for decoder in self.decoder_stack:\n",
    "            decoder_output = decoder(decoder_output, cross_input=encoder_output)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 N_enc: int,\n",
    "                 N_dec: int,\n",
    "                 vocab_size:int, \n",
    "                 d_embed: int,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 d_tgt_vocab: int,\n",
    "                 dropout_p = 0.1,\n",
    "                 dev='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.N_enc = N_enc\n",
    "        self.N_dec = N_dec\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_tgt_vocab = d_tgt_vocab\n",
    "        self.dev= dev\n",
    "\n",
    "        self.src_embedder = EmbeddingWithPositionalEncoding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_embed=self.d_embed,\n",
    "            d_model=self.d_model,\n",
    "            dropout_p=self.dropout_p,\n",
    "            dev=self.dev\n",
    "        )\n",
    "\n",
    "        self.tgt_embedder = EmbeddingWithPositionalEncoding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_embed=self.d_embed,\n",
    "            d_model=self.d_model,\n",
    "            dropout_p=self.dropout_p,\n",
    "            dev = self.dev\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder_stack = TransformerEncoderDecoder(\n",
    "            N_enc=self.N_enc,\n",
    "            N_dec=self.N_dec,\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "            dropout_p=self.dropout_p,\n",
    "            dev = self.dev\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_tgt_vocab,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_target_right(tgt_tokens: torch.Tensor, dev='cpu') -> torch.Tensor:\n",
    "        batch_size, seq_len = tgt_tokens.size() # no d_model since, no Embedding done\n",
    "        zer = torch.zeros(\n",
    "            size=(batch_size, 1),\n",
    "            device=dev,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        return torch.concat([\n",
    "            zer, \n",
    "            tgt_tokens[:, :-1]], \n",
    "            dim=1).to(dev)\n",
    "\n",
    "    def forward(self, \n",
    "                src_tokens:torch.Tensor, \n",
    "                tgt_tokens:torch.Tensor,\n",
    "                padding_mask=None\n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        tgt_tokens = self.shift_target_right(tgt_tokens, dev=self.dev) \n",
    "        # shifting is needed to prevent information leakage. \n",
    "        # it allows parallel traning in spite of hiding the token \n",
    "        # to be predicted.\n",
    "        inp_embed = self.src_embedder(src_tokens)\n",
    "        tgt_embed = self.tgt_embedder(tgt_tokens)\n",
    "\n",
    "        enc_dec_out = self.encoder_decoder_stack.forward(inp_embed, tgt_embed, padding_mask=padding_mask)\n",
    "        \n",
    "        out = self.output_proj(enc_dec_out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2448672c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='Kijai/llava-llama-3-8b-text-encoder-tokenizer',\n",
    "    use_fast=True,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22875ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Transformer(\n",
    "    N_enc= 6,\n",
    "    N_dec=6,\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_embed=1024,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    d_tgt_vocab=1024,\n",
    "    dropout_p=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dfce92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'307.859M'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0\n",
    "for param in tf.parameters():\n",
    "    p += param.numel()\n",
    "f'{round(p/1.e6, 3)}M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ab85fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_inp = tokenizer('Teja is gay')\n",
    "tgt_inp = tokenizer('Not Really')\n",
    "padding_mask = torch.tensor(src_inp['attention_mask']).unsqueeze(1).unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288095f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   6777,   5697,    374,   8485])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(src_inp['input_ids'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55147f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   2688,  29308])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(tgt_inp['input_ids'], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20a689e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_inp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtgt_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_inp\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 459\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src_tokens, tgt_tokens, padding_mask)\u001b[39m\n\u001b[32m    455\u001b[39m tgt_tokens = \u001b[38;5;28mself\u001b[39m.shift_target_right(tgt_tokens, dev=\u001b[38;5;28mself\u001b[39m.dev) \n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# shifting is needed to prevent information leakage. \u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# it allows parallel traning in spite of hiding the token \u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# to be predicted.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m inp_embed = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrc_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m tgt_embed = \u001b[38;5;28mself\u001b[39m.tgt_embedder(tgt_tokens)\n\u001b[32m    462\u001b[39m enc_dec_out = \u001b[38;5;28mself\u001b[39m.encoder_decoder_stack.forward(inp_embed, tgt_embed, padding_mask=padding_mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mEmbeddingWithPositionalEncoding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     57\u001b[39m batch_size, seq_length = x.shape\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# step 1: make embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m token_embedding = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# step 2: go from d_embed to d_model\u001b[39;00m\n\u001b[32m     63\u001b[39m token_embedding = \u001b[38;5;28mself\u001b[39m.projection(token_embedding) \\\n\u001b[32m     64\u001b[39m     * \u001b[38;5;28mself\u001b[39m.scaling \u001b[38;5;66;03m# multiplying with scaling factor, just like in the paper\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\TejaMeruva\\Attention_Is_All_I_Needed\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:2542\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2536\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2537\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2539\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2540\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2541\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2542\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "tf(\n",
    "    src_tokens=torch.tensor(tgt_inp['input_ids'], dtype=torch.long).unsqueeze(0),\n",
    "    tgt_tokens=torch.tensor(tgt_inp['input_ids'], dtype=torch.long).unsqueeze(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b5781c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0495dd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token id: 128000\n",
      "Min token id: 2688\n",
      "Model vocab size: 128000\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor(tgt_inp[\"input_ids\"], dtype=torch.long)\n",
    "\n",
    "print(\"Max token id:\", tokens.max().item())\n",
    "print(\"Min token id:\", tokens.min().item())\n",
    "print(\"Model vocab size:\", tf.src_embedder.embedding.num_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7634040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
