{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39faeef",
   "metadata": {},
   "source": [
    "# Implementing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19c11f",
   "metadata": {},
   "source": [
    "Reference: [Implementation_Tutorial](Transformer_Implementation_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b255351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7dbb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaf091",
   "metadata": {},
   "source": [
    "## Embdedding and Position Encoding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5692a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, \n",
    "                 d_embed: int, \n",
    "                 d_model: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_embed,\n",
    "            device=dev\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=d_embed,\n",
    "            out_features=d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        self.scaling = float(sqrt(self.d_model))\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    @staticmethod # decorator that indicates that the following function doesn't operate on `self`\n",
    "    def create_positional_encoding(seq_length:int, \n",
    "                                   d_model:int, \n",
    "                                   batch_size:int\n",
    "                                   ):\n",
    "\n",
    "        positions = torch.arange(seq_length, dtype=torch.long, device=dev)\\\n",
    "            .unsqueeze(1) # shape (seq_length, 1) i.e. makes it vertical\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, d_model, 2)/d_model)*(-4)*log(10)\n",
    "        ).to(dev)\n",
    "        \n",
    "        pe = torch.zeros(size=(seq_length, d_model), dtype=torch.long, device=dev) # the tensor to be multiplied to positions tensor to get pe\n",
    "        pe[:, 0::2] = torch.sin(positions*div_term) # for even dimensions\n",
    "        pe[:, 1::2] = torch.cos(positions*div_term) # for odd dimensions\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1) # copy out the encodings for each batch\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # step 1: make embeddings\n",
    "        token_embedding = self.embedding(x)\n",
    "\n",
    "        # step 2: go from d_embed to d_model\n",
    "        token_embedding = self.projection(token_embedding) \\\n",
    "            * self.scaling # multiplying with scaling factor, just like in the paper\n",
    "\n",
    "        # step 3: add positional encoding\n",
    "        pos_encoding = self.create_positional_encoding(\n",
    "            seq_length=seq_length, \n",
    "            d_model = self.d_model,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        #step 4: normalize the sum of pos encoding and token_embed\n",
    "        norm_sum = self.layerNorm(pos_encoding + token_embedding)\n",
    "        op = self.dropout(norm_sum)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafd9ac",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1a4ce",
   "metadata": {},
   "source": [
    "- Two types of attention I learnt:\n",
    "  - **Self-Attention:** key values come from the same input tensor\n",
    "  - **Cross-Attention:** key values come fromt he output of a different multi-head attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "226fb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if (d_model % num_heads) != 0: raise ValueError(f'`d_model` not divisible by `num_heads`')\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_heads = self.d_model // self.num_heads\n",
    "        self.scale_factor = float(1.0 / sqrt(self.d_heads))\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        #linear transformations\n",
    "        self.q_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.v_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                seq: torch.Tensor, \n",
    "                key_value_states:torch.Tensor = None, \n",
    "                att_mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, d_model = seq.size()\n",
    "\n",
    "        Q_state: torch.Tensor = self.q_proj(seq)\n",
    "        if key_value_states is not None:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            K_state: torch.Tensor = self.k_proj(key_value_states)\n",
    "            V_state: torch.Tensor = self.v_proj(key_value_states)\n",
    "        else:\n",
    "            kv_seq_len = seq_length\n",
    "            K_state: torch.Tensor = self.k_proj(seq)\n",
    "            V_state: torch.Tensor = self.v_proj(seq)\n",
    "\n",
    "        Q_state = Q_state.view(batch_size, seq_length, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "\n",
    "        Q_state = Q_state * self.scale_factor\n",
    "        \n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "        if att_mask is not None:\n",
    "            att = att + att_mask # yes, in this case the mask is not multiplied, but added. This is to ensure that after softmax the things to be excluded are 0\n",
    "        \n",
    "        att_score = F.softmax(self.att_matrix, dim=-1) # torch.nn.Softmax() is used in __init__, F.softmax() is used for these inline operations.\n",
    "        att_score = self.dropout(att_score)\n",
    "        att_op = torch.matmul(att_score, V_state)\n",
    "\n",
    "        #concatenating all heads \n",
    "        att_op = att_op.transpose(1, 2)\n",
    "        att_op = att_op.contiguous().view(batch_size, seq_length, self.num_heads*self.d_heads)\n",
    "\n",
    "        att_op = self.output_proj(att_op)\n",
    "\n",
    "        return att_op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f75cd",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0e6bedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3663e-01,  1.7272e-01, -3.8721e-01,  1.1135e-01, -5.6162e-03,\n",
       "           2.7718e-01, -2.6944e-01,  3.0791e-01,  6.9384e-02,  1.1480e-01,\n",
       "           2.6803e-02, -1.0110e-01,  5.3058e-02,  4.6782e-02,  3.4256e-01,\n",
       "           3.2629e-01,  2.2553e-01,  2.3477e-02,  3.6560e-01,  2.5196e-01,\n",
       "          -1.0592e-01, -4.7758e-01,  4.6523e-02, -2.1700e-01,  3.0365e-01,\n",
       "           1.8905e-01,  1.3182e-01, -2.1174e-01,  2.8109e-01, -1.6727e-02,\n",
       "           1.6279e-01,  1.3043e-01,  2.8903e-01, -3.8033e-01, -4.1511e-02,\n",
       "           1.8432e-01,  5.2712e-01,  2.4933e-01, -4.0932e-01, -7.4459e-03,\n",
       "          -2.3427e-01,  1.5465e-01,  1.8812e-01,  1.6551e-01,  1.7161e-01,\n",
       "           1.4275e-02, -4.1186e-03,  3.1627e-02,  1.5110e-01,  2.0662e-01,\n",
       "           1.9985e-01,  1.1315e-01, -3.1759e-01,  4.6601e-01, -3.9809e-01,\n",
       "          -7.4844e-02, -1.2481e-01, -1.2842e-01, -9.1034e-02, -2.1741e-01,\n",
       "          -1.0017e-01, -6.9226e-02,  7.2272e-02,  2.8972e-02, -7.2722e-02,\n",
       "          -4.0630e-02,  2.1328e-01,  1.8962e-01, -3.3998e-01,  2.3443e-01,\n",
       "           9.0877e-03, -4.7238e-02, -5.7501e-02, -2.5268e-01,  2.4717e-01,\n",
       "           3.8658e-01, -3.3210e-02,  5.6656e-02,  1.1557e-01,  5.0952e-01,\n",
       "          -1.4648e-01, -3.0597e-01,  3.2714e-01,  4.6550e-02, -2.1619e-01,\n",
       "           2.1016e-01,  1.3976e-01, -1.0960e-01, -2.0754e-02,  1.9386e-01,\n",
       "           1.9767e-01, -1.6996e-01, -6.6979e-02, -3.8804e-02,  3.1088e-01,\n",
       "          -1.6121e-02,  2.0217e-01, -3.1032e-01,  3.2337e-01, -1.9774e-01,\n",
       "           3.6374e-01, -3.8268e-02, -4.9809e-01,  4.7783e-01, -3.2526e-02,\n",
       "           2.2827e-02,  3.5752e-01, -2.4197e-01, -2.0387e-03,  4.3306e-01,\n",
       "          -1.9242e-01,  8.2680e-02, -9.5555e-02,  1.8865e-01, -2.6254e-02,\n",
       "          -5.3017e-02,  7.0513e-02, -3.1236e-01,  1.6055e-01,  2.4182e-02,\n",
       "          -4.5411e-02, -4.4500e-02, -6.6915e-02,  1.6875e-01,  1.3255e-01,\n",
       "           2.2141e-01,  4.1942e-02, -7.0746e-01,  6.0729e-02,  2.9110e-01,\n",
       "           1.3522e-01,  8.9540e-02, -1.3154e-01, -6.3596e-01, -1.5634e-01,\n",
       "           3.8395e-02, -2.5350e-01, -2.4503e-02,  4.0962e-02,  5.4373e-02,\n",
       "          -5.1138e-02, -1.4366e-01, -7.4786e-02,  1.2758e-01,  1.3629e-01,\n",
       "          -2.0051e-01,  1.7466e-04, -3.3186e-01, -8.3614e-02, -1.3525e-01,\n",
       "           1.5662e-01,  1.7136e-01,  1.1122e-01,  4.0659e-03, -2.5217e-01,\n",
       "           1.2405e-01,  2.7793e-01,  1.0820e-02,  2.9475e-01, -3.8646e-01,\n",
       "           1.7229e-01, -6.7507e-02,  2.3596e-01,  8.1728e-02, -6.6184e-02,\n",
       "           2.0230e-02,  2.6746e-02, -5.5875e-01,  5.6730e-02,  1.7050e-01,\n",
       "          -4.4748e-01,  5.8338e-01,  2.1379e-01,  7.9794e-02,  2.6681e-01,\n",
       "           7.9490e-02, -4.3768e-02,  5.4162e-03, -3.6866e-02,  1.7359e-01,\n",
       "           5.0453e-02, -7.1231e-02,  2.4591e-01, -2.2471e-01, -1.5449e-01,\n",
       "          -5.7562e-01, -3.7143e-01,  1.1579e-01, -7.6254e-02,  2.3705e-02,\n",
       "           1.7850e-01, -2.6479e-02,  4.3273e-01, -9.5539e-02,  1.5248e-02,\n",
       "           1.9134e-02, -2.2455e-01,  3.6785e-01, -6.9131e-02,  2.7800e-01,\n",
       "           1.0894e-02,  1.3300e-01, -2.6091e-01,  4.6462e-01, -1.7739e-01,\n",
       "          -2.1972e-01,  4.0744e-01, -1.7614e-01, -6.4182e-02,  1.6005e-01,\n",
       "          -2.1969e-01,  5.5945e-01, -3.5141e-01,  2.4328e-01,  2.7904e-01,\n",
       "           2.3177e-01, -2.1328e-01, -7.4452e-02, -2.0956e-01, -1.0101e-01,\n",
       "          -5.5482e-02,  2.9845e-01, -2.9366e-01,  2.4362e-01,  1.7345e-01,\n",
       "          -5.4133e-03,  1.1513e-01,  9.0659e-03, -4.5395e-02,  4.1891e-02,\n",
       "          -1.7624e-01,  4.9172e-01, -5.7876e-02, -1.2079e-01, -4.8202e-03,\n",
       "           7.9908e-02, -3.4790e-02, -4.9136e-02, -3.4068e-01, -4.1130e-02,\n",
       "          -4.7136e-01,  2.8935e-01, -1.8608e-01,  8.9705e-02,  1.0291e-01,\n",
       "           1.1714e-01, -2.1573e-02, -3.8074e-03,  3.2409e-01,  1.0851e-02,\n",
       "           4.3648e-02,  2.1639e-01,  1.9094e-01, -1.5291e-01,  3.2589e-01,\n",
       "           5.9320e-01],\n",
       "         [-7.8993e-02,  2.4347e-01, -4.3142e-01,  1.8078e-01,  9.8437e-02,\n",
       "           2.3206e-01, -2.3149e-01,  4.4398e-01, -1.3691e-01,  1.0261e-01,\n",
       "           1.2779e-01,  1.4110e-02, -1.5020e-02,  1.8191e-01,  3.2311e-01,\n",
       "           2.9504e-01,  1.5544e-01, -6.9156e-03,  2.3773e-01,  3.0503e-01,\n",
       "          -1.0266e-01, -4.0621e-01,  1.0817e-01, -2.6816e-01,  1.7867e-01,\n",
       "           1.3749e-01,  2.7637e-01, -3.4737e-02,  1.7398e-01, -2.5794e-01,\n",
       "          -1.3679e-02,  5.7024e-02,  3.7816e-01, -5.0158e-01,  2.1239e-01,\n",
       "           2.8173e-01,  4.2666e-01,  1.1460e-01, -4.1245e-01, -4.5918e-02,\n",
       "          -1.4174e-01, -1.1674e-01,  2.3802e-01,  2.3392e-01,  2.0558e-01,\n",
       "           2.3599e-03, -1.2211e-02,  1.9611e-01,  7.9085e-02,  2.9793e-01,\n",
       "           1.2750e-01,  2.5677e-01, -1.4802e-01,  3.7853e-01, -2.6586e-01,\n",
       "          -8.4951e-02, -2.4006e-02, -6.2919e-02, -1.4730e-01, -6.8892e-02,\n",
       "          -2.3018e-01, -5.8422e-02, -1.4128e-01,  1.6721e-01,  1.4617e-01,\n",
       "          -9.2688e-02,  1.1510e-01,  2.1401e-01, -1.7778e-01,  1.5782e-01,\n",
       "           7.0375e-02, -1.1524e-01,  4.6431e-02, -4.1855e-01,  3.9299e-01,\n",
       "           5.7664e-01,  1.5073e-01,  9.5509e-02, -4.1822e-02,  2.4224e-01,\n",
       "           9.6381e-02, -2.0911e-01,  1.9206e-01,  6.7634e-02, -1.3215e-01,\n",
       "           1.3097e-01, -4.0574e-02,  4.5176e-02, -2.2845e-01, -5.3787e-02,\n",
       "           1.4903e-01, -2.7295e-01, -7.8996e-02, -1.2032e-01,  6.7155e-02,\n",
       "           1.0543e-01,  1.4477e-01, -1.9693e-01,  2.7147e-01, -1.6471e-01,\n",
       "           4.8619e-01,  1.8932e-02, -4.8930e-01,  4.0122e-01,  1.0015e-01,\n",
       "           1.3418e-01,  8.5424e-02, -1.5088e-01,  5.9723e-02,  3.4029e-01,\n",
       "           1.0513e-01, -6.1600e-02, -1.2779e-01,  1.4248e-01,  1.1263e-01,\n",
       "           8.7030e-02, -1.0524e-02, -3.0925e-01,  3.7486e-02, -1.0322e-01,\n",
       "          -1.6095e-03, -1.1663e-02,  4.9178e-03,  4.1691e-01,  7.4210e-02,\n",
       "           1.3553e-01, -8.5845e-02, -6.1465e-01,  7.7417e-02,  2.7850e-01,\n",
       "           1.8257e-01,  2.0135e-01, -2.9220e-02, -3.0398e-01, -2.7641e-01,\n",
       "          -7.5393e-02, -3.0039e-01,  2.1315e-03, -9.5724e-02,  2.3128e-01,\n",
       "           3.5571e-02, -1.2537e-01, -1.2349e-01,  2.1164e-01,  4.1842e-03,\n",
       "          -1.4484e-01,  1.9076e-02, -2.1019e-01, -9.7988e-02, -2.3309e-01,\n",
       "           1.5736e-01,  2.9385e-01,  1.7315e-01, -1.0229e-01, -3.5066e-01,\n",
       "           1.9450e-01,  2.9816e-01,  1.5303e-01,  1.9834e-01, -3.6226e-01,\n",
       "           8.1288e-02,  1.0022e-02,  3.9883e-01,  9.8027e-04,  3.3898e-02,\n",
       "           1.8067e-01, -5.8554e-02, -4.2961e-01,  2.3816e-02,  1.3962e-01,\n",
       "          -4.6291e-01,  3.9467e-01,  1.5780e-01,  1.0786e-01,  1.7886e-01,\n",
       "           9.0036e-02, -6.8099e-02, -1.2909e-01,  1.7485e-02,  3.4330e-01,\n",
       "          -9.0114e-02, -4.6251e-02,  2.2747e-01, -2.9370e-01,  5.5299e-02,\n",
       "          -6.5929e-01, -3.0136e-01,  1.0686e-01,  5.3441e-02, -2.9727e-02,\n",
       "           2.1747e-02, -3.4895e-01,  6.2974e-01,  3.9179e-02,  8.6574e-03,\n",
       "           1.9405e-01, -6.0383e-02,  1.6236e-01, -1.4799e-02,  4.1506e-01,\n",
       "          -1.1570e-01,  1.5302e-01, -2.3718e-01,  3.8683e-01, -2.5730e-01,\n",
       "          -1.0670e-01,  6.4327e-01, -3.4927e-02, -1.6681e-01,  1.9337e-01,\n",
       "          -2.2060e-01,  7.8904e-01, -3.1735e-01,  5.4231e-02,  1.9409e-01,\n",
       "           1.5988e-01, -2.3521e-01, -3.5090e-02, -2.0425e-01, -2.3799e-01,\n",
       "          -1.5298e-01,  1.2305e-01, -4.2139e-01, -8.6997e-03,  1.5766e-01,\n",
       "          -3.4072e-02, -2.2907e-01, -1.6946e-01,  1.7600e-01,  4.3108e-02,\n",
       "          -6.9143e-02,  3.8412e-01, -7.6680e-02, -2.2183e-01,  1.3739e-01,\n",
       "           1.7466e-01, -1.0164e-01,  9.3708e-02, -1.3608e-01,  1.5942e-03,\n",
       "          -4.7452e-01,  2.3357e-01, -4.5106e-02,  1.1592e-01,  2.8149e-02,\n",
       "           1.0820e-01, -1.2456e-01, -3.5498e-01,  3.4002e-01,  6.3418e-02,\n",
       "          -1.3821e-01,  2.3630e-01,  1.0646e-01,  5.3466e-02,  2.7517e-01,\n",
       "           5.3188e-01],\n",
       "         [-2.1887e-01,  3.0270e-01, -4.0312e-01,  1.0851e-01,  3.1055e-01,\n",
       "           3.4445e-01, -2.4526e-01,  2.5765e-01, -1.7763e-01,  2.2082e-01,\n",
       "           2.5921e-02, -1.4604e-01, -8.7739e-02,  1.0803e-02,  3.2130e-01,\n",
       "           5.1254e-01,  7.8649e-02, -1.6116e-01,  2.6632e-01,  2.3292e-01,\n",
       "          -1.1516e-01, -3.9740e-01,  6.5732e-02, -1.4443e-01,  2.0730e-01,\n",
       "           2.8530e-01,  1.6537e-01, -5.7722e-02,  2.8788e-01, -1.6366e-01,\n",
       "           3.9489e-02,  7.2379e-02,  2.8629e-01, -4.2919e-01,  9.6244e-02,\n",
       "           2.2726e-01,  5.4779e-01,  2.3565e-01, -4.2790e-01, -3.4326e-02,\n",
       "          -2.1651e-01,  1.0371e-01,  1.2492e-01,  1.5996e-01,  1.5730e-01,\n",
       "          -2.4108e-02, -1.6269e-01,  1.4678e-01,  5.3933e-02,  2.2248e-01,\n",
       "           1.7856e-01,  1.9215e-01, -2.6774e-01,  4.1103e-01, -3.4725e-01,\n",
       "          -1.2708e-01, -7.0485e-02, -8.4340e-02, -8.7637e-02, -1.1318e-01,\n",
       "          -1.8479e-01, -7.0193e-02,  2.1843e-02,  1.3024e-01,  2.2641e-02,\n",
       "          -9.9644e-02,  8.5106e-02,  2.3411e-01, -2.2726e-01,  2.8343e-01,\n",
       "           8.2471e-02, -7.8618e-02,  1.3268e-02, -3.4349e-01,  4.4569e-01,\n",
       "           7.9249e-01,  1.6153e-01,  1.3464e-01,  3.5000e-02,  3.1959e-01,\n",
       "           8.5999e-02, -2.6120e-01,  2.9699e-01,  4.4689e-02, -2.5444e-01,\n",
       "           1.3039e-01,  8.6369e-02, -8.5504e-02, -4.3853e-02, -9.5087e-03,\n",
       "           3.6926e-01, -1.3464e-01, -8.2248e-02, -1.6314e-01,  1.6637e-01,\n",
       "           4.3325e-02,  2.3874e-01, -1.5967e-01,  2.7455e-01, -1.3967e-01,\n",
       "           4.8990e-01,  8.9596e-02, -5.4571e-01,  3.4583e-01, -1.2429e-02,\n",
       "           1.8843e-01,  2.1049e-01, -3.1449e-01,  8.1265e-02,  2.3522e-01,\n",
       "           6.7505e-02,  9.7775e-02, -1.2165e-01,  2.8274e-01, -1.3776e-02,\n",
       "           8.6170e-02, -6.4146e-02, -3.6833e-01,  1.5508e-01, -1.1286e-01,\n",
       "          -9.8474e-02,  6.4531e-02,  1.0647e-01,  2.3741e-01, -2.1972e-02,\n",
       "          -1.2965e-03, -4.7199e-02, -7.6586e-01, -1.8798e-02,  2.0100e-01,\n",
       "           2.2709e-01,  1.4709e-01, -3.9340e-02, -6.2204e-01, -1.6810e-01,\n",
       "          -1.4319e-01, -2.9742e-01,  2.1072e-01, -1.0877e-01,  1.9174e-01,\n",
       "           4.6784e-02, -1.1355e-01, -5.6516e-02,  1.3879e-01,  1.3648e-01,\n",
       "          -5.6670e-02, -3.7951e-02, -2.1550e-01, -1.7979e-01, -2.2607e-01,\n",
       "           1.3229e-01,  2.4016e-01,  1.1394e-01, -1.8625e-02, -2.9738e-01,\n",
       "           3.8457e-02,  2.7187e-01,  3.2777e-03,  1.5273e-01, -2.9829e-01,\n",
       "           7.5620e-02, -2.5366e-02,  3.6331e-01,  2.9940e-02,  7.4609e-02,\n",
       "           1.0889e-01, -8.7227e-02, -4.8668e-01, -4.7253e-02,  9.6993e-02,\n",
       "          -5.2050e-01,  6.1957e-01,  2.1946e-01,  1.5898e-01,  2.5952e-01,\n",
       "           1.3528e-01, -8.0572e-02, -6.9955e-02, -9.8741e-02,  2.1384e-01,\n",
       "           5.3326e-02, -8.9164e-02,  2.8619e-01, -2.0703e-01, -1.7757e-01,\n",
       "          -6.3449e-01, -3.7185e-01,  8.8687e-02, -3.2582e-02,  1.4133e-02,\n",
       "           1.1955e-01, -2.1492e-01,  5.4401e-01, -4.7318e-02, -1.0894e-01,\n",
       "           2.2621e-01, -2.4443e-01,  3.2836e-01,  9.6034e-02,  2.4763e-01,\n",
       "           4.9422e-02,  2.1935e-01, -1.9262e-01,  4.6625e-01, -2.4394e-01,\n",
       "          -1.5431e-01,  5.6710e-01, -7.9439e-02, -1.3725e-01,  1.6256e-01,\n",
       "          -2.5629e-01,  5.8381e-01, -3.9188e-01,  1.2877e-01,  1.9392e-01,\n",
       "           2.3748e-01, -3.2840e-01, -5.3064e-02, -2.3688e-01, -2.3752e-01,\n",
       "          -3.6886e-04,  3.0450e-01, -4.4379e-01,  1.0020e-01,  9.7985e-02,\n",
       "           1.0813e-02, -4.6409e-02, -2.0201e-01,  1.0356e-01,  5.4714e-02,\n",
       "          -1.9797e-01,  5.8576e-01, -6.7351e-02, -1.2288e-01,  1.0612e-01,\n",
       "           2.5870e-01, -1.3153e-02, -5.4268e-02, -2.0695e-01,  6.9253e-02,\n",
       "          -3.9536e-01,  1.7091e-01, -1.6727e-01, -8.2865e-04,  7.3424e-02,\n",
       "           6.7345e-02, -1.0372e-01, -1.9012e-01,  3.3740e-01, -1.4387e-01,\n",
       "          -9.8465e-02,  1.8662e-01,  1.7141e-01, -5.6084e-02,  2.0234e-01,\n",
       "           4.6281e-01]]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=512, \n",
    "    d_model=256\n",
    ")\n",
    "\n",
    "att_layer = TransformerAttention(\n",
    "    d_model=256, \n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "x = enc(torch.tensor([1, 2, 3], device=dev).unsqueeze(0))\n",
    "att_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065acf4",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025690",
   "metadata": {},
   "source": [
    "- According, to section 3.3 of the paper, this has 2 layers\n",
    "- d_model -> d_ff -> d_model\n",
    "- same parameters for every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cdf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_ff,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=self.d_ff,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.Tensor):\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "        f2 = self.fc1(f1)\n",
    "        return f2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
