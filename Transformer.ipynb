{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39faeef",
   "metadata": {},
   "source": [
    "# Implementing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19c11f",
   "metadata": {},
   "source": [
    "Reference: [Implementation_Tutorial](Transformer_Implementation_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b255351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7dbb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaf091",
   "metadata": {},
   "source": [
    "## Embdedding and Position Encoding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5692a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, \n",
    "                 d_embed: int, \n",
    "                 d_model: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_embed,\n",
    "            device=dev\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=d_embed,\n",
    "            out_features=d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        self.scaling = float(sqrt(self.d_model))\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    @staticmethod # decorator that indicates that the following function doesn't operate on `self`\n",
    "    def create_positional_encoding(seq_length:int, \n",
    "                                   d_model:int, \n",
    "                                   batch_size:int\n",
    "                                   ):\n",
    "\n",
    "        positions = torch.arange(seq_length, dtype=torch.long, device=dev)\\\n",
    "            .unsqueeze(1) # shape (seq_length, 1) i.e. makes it vertical\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, d_model, 2)/d_model)*(-4)*log(10)\n",
    "        ).to(dev)\n",
    "        \n",
    "        pe = torch.zeros(size=(seq_length, d_model), dtype=torch.long, device=dev) # the tensor to be multiplied to positions tensor to get pe\n",
    "        pe[:, 0::2] = torch.sin(positions*div_term) # for even dimensions\n",
    "        pe[:, 1::2] = torch.cos(positions*div_term) # for odd dimensions\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1) # copy out the encodings for each batch\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # step 1: make embeddings\n",
    "        token_embedding = self.embedding(x)\n",
    "\n",
    "        # step 2: go from d_embed to d_model\n",
    "        token_embedding = self.projection(token_embedding) \\\n",
    "            * self.scaling # multiplying with scaling factor, just like in the paper\n",
    "\n",
    "        # step 3: add positional encoding\n",
    "        pos_encoding = self.create_positional_encoding(\n",
    "            seq_length=seq_length, \n",
    "            d_model = self.d_model,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        #step 4: normalize the sum of pos encoding and token_embed\n",
    "        norm_sum = self.layerNorm(pos_encoding + token_embedding)\n",
    "        op = self.dropout(norm_sum)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafd9ac",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1a4ce",
   "metadata": {},
   "source": [
    "- Two types of attention I learnt:\n",
    "  - **Self-Attention:** key values come from the same input tensor\n",
    "  - **Cross-Attention:** key values come fromt he output of a different multi-head attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "226fb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 dropout_p: float = 0.1,\n",
    "                 dev='cpu'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if (d_model % num_heads) != 0: raise ValueError(f'`d_model` not divisible by `num_heads`')\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_heads = self.d_model // self.num_heads\n",
    "        self.scale_factor = float(1.0 / sqrt(self.d_heads))\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.dev = dev\n",
    "\n",
    "        #linear transformations\n",
    "        self.q_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=self.dev\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.v_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                seq: torch.Tensor, \n",
    "                key_value_states:torch.Tensor = None, \n",
    "                att_mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, d_model = seq.size()\n",
    "\n",
    "        Q_state: torch.Tensor = self.q_proj(seq)\n",
    "        if key_value_states is not None:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            K_state: torch.Tensor = self.k_proj(key_value_states)\n",
    "            V_state: torch.Tensor = self.v_proj(key_value_states)\n",
    "        else:\n",
    "            kv_seq_len = seq_length\n",
    "            K_state: torch.Tensor = self.k_proj(seq)\n",
    "            V_state: torch.Tensor = self.v_proj(seq)\n",
    "\n",
    "        Q_state = Q_state.view(batch_size, seq_length, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "\n",
    "        Q_state = Q_state * self.scale_factor\n",
    "        \n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask # yes, in this case the mask is not multiplied, but added. This is to ensure that after softmax the things to be excluded are 0\n",
    "        \n",
    "        att_score = F.softmax(self.att_matrix, dim=-1) # torch.nn.Softmax() is used in __init__, F.softmax() is used for these inline operations.\n",
    "        att_score = self.dropout(att_score)\n",
    "        att_op = torch.matmul(att_score, V_state)\n",
    "\n",
    "        #concatenating all heads \n",
    "        att_op = att_op.transpose(1, 2)\n",
    "        att_op = att_op.contiguous().view(batch_size, seq_length, self.num_heads*self.d_heads)\n",
    "\n",
    "        att_op = self.output_proj(att_op)\n",
    "\n",
    "        return att_op\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6bedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.2453e-01,  1.5356e-03,  9.8504e-02, -3.6989e-02, -2.0104e-01,\n",
       "           3.0645e-02, -4.8723e-01, -2.8341e-01,  7.7438e-02,  2.9908e-01,\n",
       "          -1.4741e-01,  1.4592e-01,  1.7704e-01,  1.1769e-01,  4.1054e-02,\n",
       "          -2.1217e-01, -3.8033e-01,  1.4640e-01,  5.0318e-02,  5.0131e-01,\n",
       "           5.9424e-01,  3.8934e-01, -2.9320e-02,  3.0623e-01, -1.8369e-01,\n",
       "          -7.2738e-02,  2.0785e-01,  4.2893e-01,  3.0858e-01,  3.7215e-01,\n",
       "          -6.4216e-01, -2.8656e-01,  4.6259e-01, -1.7676e-01,  3.0973e-01,\n",
       "          -1.8818e-01,  8.9793e-02, -5.8858e-01, -8.1687e-02,  1.2802e-01,\n",
       "          -7.9837e-02, -1.0723e-01,  7.6052e-02, -5.3435e-01,  2.9916e-01,\n",
       "          -2.1117e-01, -6.2239e-01,  3.0656e-02,  1.9151e-01,  3.8035e-01,\n",
       "           1.0984e-01,  2.0858e-01,  8.7536e-02,  3.2996e-01, -2.9303e-01,\n",
       "          -1.1950e-01,  3.0866e-02,  2.6978e-01,  5.3533e-02,  4.4368e-01,\n",
       "           2.6899e-01,  2.2969e-02, -1.1797e-01,  4.2570e-02, -2.7551e-01,\n",
       "          -4.7569e-01, -5.5470e-01,  3.0267e-02, -2.2417e-01,  2.1560e-01,\n",
       "          -6.4312e-01, -8.6289e-03,  6.7161e-02, -2.2915e-02,  5.5280e-01,\n",
       "           4.1580e-01, -2.9559e-01, -8.3964e-02,  6.6235e-01,  3.4714e-01,\n",
       "           1.3663e-01,  2.6578e-01, -1.3678e-01,  1.0856e-01, -1.9006e-01,\n",
       "           1.2333e-01,  4.5462e-01, -1.8151e-01,  3.2923e-01, -3.0754e-01,\n",
       "          -1.8639e-01,  2.7408e-01,  1.8621e-01,  2.7757e-01,  7.6973e-02,\n",
       "           1.6542e-01,  4.8203e-01,  5.0183e-01,  1.4495e-01,  5.3893e-01,\n",
       "          -2.1577e-01, -3.9045e-01,  3.9667e-01, -1.3839e-01, -9.2496e-02,\n",
       "           2.6752e-01, -6.9186e-02,  1.9490e-01,  1.2816e-01,  2.6044e-01,\n",
       "          -2.9714e-01,  4.5137e-01,  9.8798e-02, -6.6197e-03, -2.5035e-01,\n",
       "           7.2034e-02, -5.1291e-02,  2.0101e-02,  7.8760e-02,  2.0696e-01,\n",
       "          -2.7236e-01, -5.0497e-02, -9.5306e-02, -4.9417e-01,  2.4863e-01,\n",
       "          -6.1778e-02, -1.0087e-01,  2.6809e-02, -5.0752e-01,  1.3910e-01,\n",
       "           2.3496e-01,  1.7331e-03, -1.3893e-01, -9.7530e-02,  3.5086e-01,\n",
       "          -1.6330e-01, -2.6703e-02,  3.1332e-01,  1.5392e-01, -4.2055e-01,\n",
       "          -5.3319e-01, -6.1764e-02, -2.1219e-01,  1.8368e-01, -1.2681e-01,\n",
       "          -4.7183e-02,  2.1280e-01, -4.8931e-01, -1.8256e-01, -4.1661e-01,\n",
       "           5.1556e-02, -3.9353e-01, -2.2977e-01,  5.9983e-02, -2.0341e-01,\n",
       "           4.2430e-02,  4.1400e-01, -4.1750e-02,  4.5566e-01, -6.5780e-01,\n",
       "           1.3564e-01, -2.2001e-01,  3.2869e-01,  4.6659e-01, -5.7096e-02,\n",
       "          -1.4057e-01, -3.3896e-01,  8.5126e-02,  1.1452e-01, -4.8062e-02,\n",
       "          -1.8155e-01, -1.5941e-01, -1.0226e-02, -2.1768e-01,  8.5658e-02,\n",
       "           1.8953e-01, -2.6893e-01, -2.0222e-01,  9.8105e-03,  1.0646e-01,\n",
       "           3.2593e-01, -4.6607e-02, -5.0603e-01, -3.3843e-01, -9.8663e-02,\n",
       "          -3.2222e-01,  6.6496e-02, -1.3376e-01, -2.3338e-01,  4.2700e-01,\n",
       "           4.7497e-01, -2.5089e-01,  4.4722e-02,  6.7611e-02,  5.3885e-01,\n",
       "          -1.7630e-01, -1.2872e-01,  2.6092e-01, -6.6995e-03,  1.8886e-01,\n",
       "          -5.1135e-01,  2.3581e-02,  1.4262e-01,  1.8624e-02, -6.6964e-03,\n",
       "          -1.0824e-01, -3.7042e-03, -2.1739e-01, -5.6319e-04, -1.6020e-01,\n",
       "           8.1966e-02, -1.5411e-01,  2.5965e-02, -3.9925e-02, -1.0402e-01,\n",
       "          -2.7270e-01,  3.7946e-01,  4.0899e-01, -1.5048e-01,  1.1630e-01,\n",
       "           5.2689e-01,  1.0831e-01,  8.2715e-02,  7.9892e-03,  7.8393e-02,\n",
       "          -9.1776e-02, -6.6775e-02,  2.1855e-01, -2.1160e-01, -2.3661e-01,\n",
       "          -1.5190e-01,  3.5212e-01,  5.1402e-02,  1.6457e-03,  1.1265e-01,\n",
       "          -2.5096e-01, -3.7625e-01, -3.0304e-01, -2.0535e-01, -1.9283e-01,\n",
       "           2.7896e-01,  3.0037e-01, -1.8511e-01,  9.2929e-02, -1.9285e-01,\n",
       "           3.1204e-01, -4.9483e-02,  1.0926e-02,  4.1371e-01,  1.8735e-01,\n",
       "           3.7427e-01,  3.2887e-01, -1.9654e-01,  5.7114e-02,  1.6686e-01,\n",
       "           3.0559e-01],\n",
       "         [ 2.3572e-02, -1.4570e-01,  1.1540e-01, -6.8676e-02, -1.3504e-01,\n",
       "           5.0257e-02, -5.1382e-01, -1.3032e-01,  6.4668e-02,  3.4031e-01,\n",
       "          -2.5110e-01, -2.4441e-02,  7.3586e-02,  3.8173e-01, -8.0493e-02,\n",
       "          -1.9812e-01, -1.1520e-01,  1.6055e-01, -5.3716e-02,  4.0577e-01,\n",
       "           4.1279e-01,  2.0345e-01, -6.2835e-02,  2.5046e-01, -3.7992e-01,\n",
       "          -2.8465e-01,  2.5790e-01,  3.6940e-01,  7.1973e-02,  3.2028e-01,\n",
       "          -4.9217e-01, -2.3585e-01,  4.1821e-01, -5.9751e-02,  1.4288e-01,\n",
       "          -2.3245e-01, -1.2530e-02, -5.8063e-01, -1.2498e-01,  1.0433e-01,\n",
       "          -2.5883e-01, -1.4032e-02,  9.9273e-03, -5.1664e-01,  3.1806e-01,\n",
       "          -2.6884e-01, -4.0027e-01,  1.3884e-02,  1.8324e-01,  2.7464e-01,\n",
       "           2.8514e-01,  2.7291e-01, -2.2261e-02,  3.0681e-01, -4.0250e-01,\n",
       "          -7.2519e-02,  5.8400e-02,  2.7412e-01,  2.0582e-01,  4.1394e-01,\n",
       "          -3.0360e-02, -4.1199e-02, -1.0711e-01,  8.1055e-02, -3.4520e-01,\n",
       "          -6.0848e-01, -4.4392e-01,  2.0376e-01, -1.5655e-01,  1.5803e-01,\n",
       "          -3.2569e-01,  7.8041e-02,  6.5696e-02, -4.7457e-02,  5.2181e-01,\n",
       "           2.3693e-01, -2.1676e-01, -1.5214e-02,  5.1321e-01,  1.0673e-01,\n",
       "          -8.2812e-02,  9.7415e-02,  3.4612e-02,  1.4541e-01, -1.9088e-01,\n",
       "           1.4698e-01,  3.4974e-01, -5.9643e-02,  3.8073e-02, -2.7164e-01,\n",
       "          -3.0206e-01,  1.6340e-01,  1.5865e-01,  1.3571e-01,  1.3988e-01,\n",
       "           1.2430e-01,  2.9304e-01,  4.0954e-01,  1.3777e-01,  5.0231e-01,\n",
       "          -1.9272e-01, -2.4744e-01,  3.0775e-01,  4.8022e-02,  3.4535e-02,\n",
       "           1.3040e-01, -1.5155e-01,  4.1731e-02,  2.6295e-01,  1.9450e-01,\n",
       "          -3.1428e-01,  2.6413e-01,  8.8249e-02,  1.8063e-01, -1.5268e-01,\n",
       "          -7.2740e-02, -5.9066e-02,  4.4890e-02,  7.3729e-02,  3.2971e-01,\n",
       "          -2.9720e-01,  8.7332e-02, -1.3349e-01, -2.8132e-01,  1.8777e-01,\n",
       "           4.6243e-02, -2.3868e-01,  8.8195e-02, -3.1552e-01,  9.0776e-02,\n",
       "           1.3118e-01, -3.0630e-02, -1.7890e-01, -2.4346e-01,  3.0881e-01,\n",
       "          -1.1276e-01, -1.8674e-01,  5.4546e-02,  1.0683e-01, -2.5552e-01,\n",
       "          -3.9069e-01, -1.1703e-01, -2.2583e-01,  2.5265e-01, -9.7991e-02,\n",
       "          -6.8520e-02,  5.5967e-02, -5.9984e-01, -6.0273e-02, -3.1801e-01,\n",
       "           7.5321e-02, -3.1675e-01, -1.2912e-01, -1.0037e-01, -1.8263e-01,\n",
       "           1.2285e-01,  2.9748e-01,  1.4254e-02,  4.4911e-01, -4.2562e-01,\n",
       "           1.2373e-02, -2.6025e-01,  2.8934e-01,  2.8816e-01, -1.0751e-01,\n",
       "          -1.0467e-01, -3.3368e-01, -7.8545e-02,  1.2166e-01, -9.0489e-02,\n",
       "          -1.4239e-01, -1.8931e-01,  1.2455e-01, -2.5725e-01,  4.0504e-03,\n",
       "           2.0646e-01, -3.7725e-01, -2.2882e-01,  7.9310e-02,  1.3140e-01,\n",
       "           2.7991e-01, -1.6533e-02, -4.5484e-01, -2.6076e-01, -1.4310e-01,\n",
       "          -8.0471e-02,  1.2192e-01, -9.6578e-02,  1.8016e-02,  4.8155e-01,\n",
       "           4.8433e-01, -2.3742e-01, -2.1275e-02,  6.2019e-02,  4.6038e-01,\n",
       "          -3.2391e-01,  5.6774e-02,  2.7187e-01, -2.2304e-02, -5.4693e-02,\n",
       "          -5.0847e-01,  2.5957e-02,  6.9053e-02, -7.3237e-02, -1.2000e-01,\n",
       "          -1.2846e-01,  5.9414e-03, -1.6301e-01, -1.1274e-01, -2.5074e-01,\n",
       "          -1.2807e-02, -1.8056e-01,  1.0125e-01, -6.9800e-02, -1.5586e-02,\n",
       "          -1.9675e-01,  3.4413e-01,  2.9630e-01, -9.4903e-02,  2.4659e-01,\n",
       "           5.5821e-01,  1.9411e-01, -2.6486e-02,  1.3484e-01,  1.0021e-01,\n",
       "          -2.6192e-01, -1.8890e-01,  2.1165e-01, -1.2748e-01, -2.9956e-01,\n",
       "          -1.5348e-01,  3.8545e-01,  3.0694e-02, -6.6104e-02,  2.4944e-01,\n",
       "          -1.8528e-01, -3.4849e-01, -3.2063e-01, -2.0770e-01, -1.6947e-01,\n",
       "           2.7232e-01,  3.4623e-01, -1.3644e-01,  4.2731e-02, -1.1952e-01,\n",
       "           1.2105e-01,  1.3358e-02, -1.5484e-01,  4.0890e-01,  1.4085e-01,\n",
       "           3.9466e-01,  2.4898e-01, -3.2577e-02, -1.0004e-01,  2.1824e-01,\n",
       "           2.1206e-01],\n",
       "         [ 1.6151e-02, -2.9767e-03, -5.7550e-02, -8.7671e-02, -1.6806e-01,\n",
       "          -3.6028e-02, -4.1467e-01, -1.6011e-01, -1.4870e-01,  2.1949e-01,\n",
       "          -1.5695e-01, -2.6059e-03,  5.3597e-02,  2.6556e-01, -8.8739e-03,\n",
       "          -1.8860e-01, -1.2989e-01,  2.9837e-01, -8.6563e-02,  3.3636e-01,\n",
       "           4.2984e-01,  2.1109e-01,  8.2442e-02,  4.6914e-01, -3.8456e-01,\n",
       "          -2.4630e-01,  2.5932e-01,  5.0397e-01,  1.4635e-01,  5.5964e-02,\n",
       "          -4.0277e-01, -1.8720e-01,  3.3684e-01,  8.5213e-02,  2.0212e-01,\n",
       "          -1.9739e-01,  9.9963e-02, -6.5199e-01, -2.3235e-01,  1.4227e-01,\n",
       "          -3.2391e-01,  2.0191e-01,  4.7843e-02, -4.0858e-01,  2.7136e-01,\n",
       "          -2.5119e-01, -2.6677e-01,  1.0322e-01,  1.1011e-01,  2.4477e-01,\n",
       "           4.3069e-01,  2.9864e-01,  1.3177e-01,  3.8327e-01, -3.7281e-01,\n",
       "          -1.1290e-01,  8.7278e-03,  3.1038e-01,  6.6279e-02,  3.4859e-01,\n",
       "          -1.4189e-01,  1.0503e-02, -1.5707e-01, -4.3891e-03, -2.3852e-01,\n",
       "          -6.8638e-01, -5.5696e-01,  1.7850e-01, -9.8628e-02,  2.3584e-01,\n",
       "          -2.8265e-01,  2.1346e-01,  8.2790e-02, -5.5333e-04,  5.5770e-01,\n",
       "           6.2795e-02, -2.6515e-01, -6.6185e-03,  5.0897e-01,  1.1998e-01,\n",
       "          -8.0060e-02, -1.8350e-01,  1.0594e-01,  9.3808e-02, -2.1784e-01,\n",
       "          -6.8641e-02,  1.4177e-01, -1.6324e-01,  1.5311e-01, -2.2531e-01,\n",
       "          -2.9647e-01,  4.1164e-01,  6.8683e-02,  2.8924e-01,  2.1742e-01,\n",
       "           6.6067e-02,  1.9045e-01,  4.0587e-01,  6.9560e-02,  4.9657e-01,\n",
       "          -1.6026e-01, -1.7490e-01,  2.4058e-01, -1.0301e-01, -1.0135e-01,\n",
       "           3.5836e-02, -1.6089e-01,  5.4089e-02,  2.6228e-01,  1.0629e-01,\n",
       "          -1.9885e-01,  3.8934e-01,  2.4409e-02,  1.5383e-01, -2.2451e-01,\n",
       "          -1.2441e-01, -4.1314e-02,  4.0798e-02,  7.7160e-02,  3.8273e-01,\n",
       "          -2.6810e-01,  2.1559e-01,  1.9716e-01, -2.1392e-01,  7.2932e-02,\n",
       "           7.9856e-02, -2.5044e-01, -2.3582e-02, -3.5860e-01,  2.0143e-01,\n",
       "           1.5339e-01, -6.2723e-02, -1.3064e-01, -2.2689e-01,  2.3443e-01,\n",
       "          -2.1349e-01, -3.2848e-01,  6.3897e-02,  1.9054e-02, -2.0253e-01,\n",
       "          -4.7651e-01, -5.7757e-02, -3.1205e-01,  1.8067e-01,  2.1548e-02,\n",
       "          -1.7813e-01, -4.3832e-02, -7.1976e-01, -5.9425e-02, -4.9576e-01,\n",
       "          -4.1004e-02, -3.2710e-01, -1.2165e-01, -4.9188e-03, -2.9137e-01,\n",
       "           1.2864e-01,  2.6590e-01,  1.2860e-01,  4.0105e-01, -4.8260e-01,\n",
       "           5.3554e-02, -3.2800e-01,  1.9661e-01,  3.0705e-01,  2.1326e-02,\n",
       "          -2.6754e-01, -4.8883e-01,  1.0997e-01,  5.6407e-02, -6.3091e-02,\n",
       "          -1.5658e-01, -1.5599e-01,  2.2108e-01, -2.6175e-01,  4.8185e-02,\n",
       "           8.2771e-02, -4.1963e-01, -3.7733e-01,  7.4435e-02,  1.8358e-01,\n",
       "           2.2432e-01, -4.9280e-02, -1.8053e-01, -1.8319e-01,  3.2279e-03,\n",
       "          -6.2758e-02,  1.2669e-01, -5.9944e-02,  3.7151e-02,  6.3574e-01,\n",
       "           5.0289e-01, -1.8981e-01, -9.1634e-02,  4.6573e-02,  3.1204e-01,\n",
       "          -8.9137e-02,  9.6139e-02,  1.7679e-01,  9.6754e-02, -1.4738e-01,\n",
       "          -4.6127e-01,  7.5601e-02,  4.4184e-02, -2.1417e-01, -5.6769e-02,\n",
       "           5.8091e-02,  1.2213e-01, -2.9046e-01, -2.2406e-01, -2.9180e-01,\n",
       "           9.6050e-03, -3.0056e-01,  2.9344e-01, -3.7454e-02, -2.0186e-02,\n",
       "          -2.7276e-01,  2.1793e-01,  3.6021e-01,  4.8918e-02,  2.2116e-01,\n",
       "           5.4242e-01,  5.7241e-03, -2.4992e-01,  1.3295e-01,  6.8069e-02,\n",
       "          -5.8136e-01,  2.4004e-02,  1.5785e-01, -2.0424e-01, -4.6706e-01,\n",
       "          -7.2998e-02,  2.4383e-01,  4.6094e-02, -1.5578e-01,  1.4822e-01,\n",
       "          -3.5416e-01, -5.8222e-01, -4.7733e-01, -1.3890e-01, -1.1595e-01,\n",
       "          -5.6493e-02,  3.6537e-01, -5.0253e-02,  1.2589e-01, -7.6585e-02,\n",
       "           6.2245e-02, -1.0074e-01,  7.3405e-03,  5.9172e-01,  1.2182e-01,\n",
       "           1.8650e-01,  2.1706e-01,  7.3958e-02,  1.2512e-01,  1.3102e-01,\n",
       "           1.7776e-01]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=512, \n",
    "    d_model=256\n",
    ")\n",
    "\n",
    "att_layer = TransformerAttention(\n",
    "    d_model=256, \n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "x = enc(torch.tensor([1, 2, 3], device=dev).unsqueeze(0))\n",
    "att_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065acf4",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025690",
   "metadata": {},
   "source": [
    "- According, to section 3.3 of the paper, this has 2 layers\n",
    "- d_model -> d_ff -> d_model\n",
    "- same parameters for every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cdf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_ff,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=self.d_ff,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.Tensor):\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "        f2 = self.fc2(f1)\n",
    "        return f2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d124573",
   "metadata": {},
   "source": [
    "## The Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f19136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int, \n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.att_layer = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=dev)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_att = self.att_layer(x)\n",
    "\n",
    "        x_att = self.dropout(x_att)\n",
    "        x_norm1 = self.norm1(x + x_att)\n",
    "\n",
    "        x_ff = self.ffn(x_norm1)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm2 = self.norm2(x_ff + x_norm1)\n",
    "        \n",
    "        return x_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c205fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    d_model= 512,\n",
    "    d_ff=2048,\n",
    "    num_heads=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756e27b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (att_layer): TransformerAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): FeedForwardNetwork(\n",
       "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "encoder(\n",
    "    enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0))\n",
    "    )\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538db08",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf6438",
   "metadata": {},
   "source": [
    "only the final encoder output is used for all the decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78af3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.att_layer1 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.att_layer2 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff\n",
    "        )\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=dev), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, value=float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                cross_input:torch.Tensor,\n",
    "                padding_mask:torch.Tensor = None\n",
    "                ):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        causal_mask = self.create_causal_mask(seq_len=seq_length)\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1) #unsqeeze the mast for self attention\n",
    "\n",
    "        x_att1 = self.att_layer1( #self-attention\n",
    "            seq=x,\n",
    "            att_mask = causal_mask\n",
    "        )\n",
    "\n",
    "        x_att1 = self.dropout(x_att1)\n",
    "        x_norm1 = self.norm1(x_att1 + x)\n",
    "\n",
    "        x_att2 = self.att_layer2( #cross attention\n",
    "            seq=x_norm1,\n",
    "            key_value_states=cross_input,\n",
    "            att_mask = padding_mask\n",
    "        )\n",
    "\n",
    "        x_att2 = self.dropout(x_att2)\n",
    "        x_norm2 = self.norm2(x_att2)\n",
    "\n",
    "        x_ff = self.ffn(x_norm2)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm3 = self.norm3(x_ff)\n",
    "        \n",
    "        return x_norm3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39f58b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1115, -0.3260, -0.2035,  ...,  0.3012,  0.1688, -0.1281],\n",
       "         [ 0.0558,  0.1714, -0.9992,  ...,  1.5448,  0.4007, -1.6595],\n",
       "         [-0.3475,  1.1575, -0.9196,  ...,  0.7762, -0.4351, -2.1617]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "decoder = TransformerDecoder(\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "x = encoder(\n",
    "    enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0))\n",
    "    )\n",
    "\n",
    "decoder.forward(enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0)), x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb7bdf",
   "metadata": {},
   "source": [
    "## Transformer Encoder and Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a12b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 N_enc: int,\n",
    "                 N_dec: int, \n",
    "                 d_model:int,\n",
    "                 num_heads: int, \n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.encoder_stack = nn.ModuleList([\n",
    "            TransformerEncoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                d_ff=self.d_ff,\n",
    "                dropout_p=self.dropout_p\n",
    "            ) for _ in range(N_enc)\n",
    "        ])\n",
    "\n",
    "        self.decoder_stack = nn.ModuleList([\n",
    "            TransformerDecoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                d_ff=self.d_ff,\n",
    "                dropout_p=self.dropout_p\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y:torch.Tensor) -> torch.Tensor:\n",
    "        #pass through the encoder stack\n",
    "        encoder_output = x\n",
    "        for encoder in self.encoder_stack:\n",
    "            encoder_output = encoder(encoder_output)\n",
    "\n",
    "        #pass through the decoder stack\n",
    "        #uses only the final encoder input\n",
    "        decoder_output = y\n",
    "        for decoder in self.decoder_stack:\n",
    "            decoder_output = decoder(decoder_output, cross_input=encoder_output)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6794bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9166, -1.1369,  0.7707,  ..., -0.5880, -0.4792, -0.8575],\n",
       "         [-0.6653, -1.5406, -0.7790,  ..., -1.5379,  0.0693, -0.4902],\n",
       "         [-0.2039, -2.0182,  0.8717,  ..., -1.4624, -0.3908, -0.1823]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "tf = TransformerEncoderDecoder(\n",
    "    N_enc = 6,\n",
    "    N_dec=6,\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "tf(enc(\n",
    "    torch.tensor([1, 2, 3], device=dev).unsqueeze(0)\n",
    "    ), enc(\n",
    "    torch.tensor([1, 2, 3], device=dev).unsqueeze(0)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac9b49",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "436b2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 N_enc: int,\n",
    "                 N_dec: int,\n",
    "                 vocab_size:int, \n",
    "                 d_embed: int,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 d_tgt_vocab: int,\n",
    "                 dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.N_enc = N_enc\n",
    "        self.N_dec = N_dec\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_tgt_vocab = d_tgt_vocab\n",
    "\n",
    "        self.src_embedder = EmbeddingWithPositionalEncoding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_embed=self.d_embed,\n",
    "            d_model=self.d_model,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.tgt_embedder = EmbeddingWithPositionalEncoding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_embed=self.d_embed,\n",
    "            d_model=self.d_model,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder_stack = TransformerEncoderDecoder(\n",
    "            N_enc=self.N_enc,\n",
    "            N_dec=self.N_dec,\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_tgt_vocab,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_target_right(tgt_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = tgt_tokens.size() # no d_model since, no Embedding done\n",
    "        zer = torch.zeros(\n",
    "            size=(batch_size, 1),\n",
    "            device=dev,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        return torch.concat([\n",
    "            zer, \n",
    "            tgt_tokens[:, :-1]], \n",
    "            dim=1).to(dev)\n",
    "\n",
    "    def forward(self, \n",
    "                src_tokens:torch.Tensor, \n",
    "                tgt_tokens:torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        tgt_tokens = self.shift_target_right(tgt_tokens) \n",
    "        # shifting is needed to prevent information leakage. \n",
    "        # it allows parallel traning in spite of hiding the token \n",
    "        # to be predicted.\n",
    "        inp_embed = self.src_embedder(src_tokens)\n",
    "        tgt_embed = self.tgt_embedder(tgt_tokens)\n",
    "\n",
    "        enc_dec_out = self.encoder_decoder_stack.forward(inp_embed, tgt_embed)\n",
    "        \n",
    "        out = self.output_proj(enc_dec_out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98bf88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Transformer(\n",
    "    N_enc = 6,\n",
    "    N_dec=6,\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048,\n",
    "    vocab_size=100,\n",
    "    d_embed=1024,\n",
    "    d_tgt_vocab=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27b9e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 24,426,084\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in tf.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5681c4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 100])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(\n",
    "    src_tokens=torch.tensor([1., 2., 3.], device=dev, dtype=torch.long).unsqueeze(0),\n",
    "    tgt_tokens=torch.tensor([1., 2., 3.], device=dev, dtype=torch.long).unsqueeze(0),\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d671eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
