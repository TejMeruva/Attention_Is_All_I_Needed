{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39faeef",
   "metadata": {},
   "source": [
    "# Implementing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19c11f",
   "metadata": {},
   "source": [
    "Reference: [Implementation_Tutorial](Transformer_Implementation_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b255351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7dbb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaf091",
   "metadata": {},
   "source": [
    "## Embdedding and Position Encoding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5692a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, \n",
    "                 d_embed: int, \n",
    "                 d_model: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_embed,\n",
    "            device=dev\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=d_embed,\n",
    "            out_features=d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        self.scaling = float(sqrt(self.d_model))\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    @staticmethod # decorator that indicates that the following function doesn't operate on `self`\n",
    "    def create_positional_encoding(seq_length:int, \n",
    "                                   d_model:int, \n",
    "                                   batch_size:int\n",
    "                                   ):\n",
    "\n",
    "        positions = torch.arange(seq_length, dtype=torch.long, device=dev)\\\n",
    "            .unsqueeze(1) # shape (seq_length, 1) i.e. makes it vertical\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, d_model, 2)/d_model)*(-4)*log(10)\n",
    "        ).to(dev)\n",
    "        \n",
    "        pe = torch.zeros(size=(seq_length, d_model), dtype=torch.long, device=dev) # the tensor to be multiplied to positions tensor to get pe\n",
    "        pe[:, 0::2] = torch.sin(positions*div_term) # for even dimensions\n",
    "        pe[:, 1::2] = torch.cos(positions*div_term) # for odd dimensions\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1) # copy out the encodings for each batch\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # step 1: make embeddings\n",
    "        token_embedding = self.embedding(x)\n",
    "\n",
    "        # step 2: go from d_embed to d_model\n",
    "        token_embedding = self.projection(token_embedding) \\\n",
    "            * self.scaling # multiplying with scaling factor, just like in the paper\n",
    "\n",
    "        # step 3: add positional encoding\n",
    "        pos_encoding = self.create_positional_encoding(\n",
    "            seq_length=seq_length, \n",
    "            d_model = self.d_model,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        #step 4: normalize the sum of pos encoding and token_embed\n",
    "        norm_sum = self.layerNorm(pos_encoding + token_embedding)\n",
    "        op = self.dropout(norm_sum)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafd9ac",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1a4ce",
   "metadata": {},
   "source": [
    "- Two types of attention I learnt:\n",
    "  - **Self-Attention:** key values come from the same input tensor\n",
    "  - **Cross-Attention:** key values come fromt he output of a different multi-head attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "226fb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if (d_model % num_heads) != 0: raise ValueError(f'`d_model` not divisible by `num_heads`')\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_heads = self.d_model // self.num_heads\n",
    "        self.scale_factor = float(1.0 / sqrt(self.d_heads))\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        #linear transformations\n",
    "        self.q_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.v_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                seq: torch.Tensor, \n",
    "                key_value_states:torch.Tensor = None, \n",
    "                att_mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, d_model = seq.size()\n",
    "\n",
    "        Q_state: torch.Tensor = self.q_proj(seq)\n",
    "        if key_value_states is not None:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            K_state: torch.Tensor = self.k_proj(key_value_states)\n",
    "            V_state: torch.Tensor = self.v_proj(key_value_states)\n",
    "        else:\n",
    "            kv_seq_len = seq_length\n",
    "            K_state: torch.Tensor = self.k_proj(seq)\n",
    "            V_state: torch.Tensor = self.v_proj(seq)\n",
    "\n",
    "        Q_state = Q_state.view(batch_size, seq_length, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "\n",
    "        Q_state = Q_state * self.scale_factor\n",
    "        \n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask # yes, in this case the mask is not multiplied, but added. This is to ensure that after softmax the things to be excluded are 0\n",
    "        \n",
    "        att_score = F.softmax(self.att_matrix, dim=-1) # torch.nn.Softmax() is used in __init__, F.softmax() is used for these inline operations.\n",
    "        att_score = self.dropout(att_score)\n",
    "        att_op = torch.matmul(att_score, V_state)\n",
    "\n",
    "        #concatenating all heads \n",
    "        att_op = att_op.transpose(1, 2)\n",
    "        att_op = att_op.contiguous().view(batch_size, seq_length, self.num_heads*self.d_heads)\n",
    "\n",
    "        att_op = self.output_proj(att_op)\n",
    "\n",
    "        return att_op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f75cd",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6bedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.8576e-01, -3.7510e-02, -3.2933e-02, -1.5562e-01, -1.5543e-01,\n",
       "          -9.6675e-02, -8.3409e-02, -4.0597e-01, -2.9541e-01, -2.0960e-01,\n",
       "          -3.6246e-01, -3.4334e-01, -4.5305e-02, -1.9015e-01, -1.8598e-01,\n",
       "          -2.7759e-01,  6.2402e-01,  9.8864e-02,  3.1582e-01, -1.7067e-02,\n",
       "          -8.0224e-02, -1.5658e-01, -5.4462e-02,  5.5463e-02,  9.8601e-02,\n",
       "           3.6265e-01, -5.2200e-01, -1.9980e-01, -2.0509e-02, -1.6520e-01,\n",
       "           1.5930e-01,  7.7368e-02, -3.3910e-02, -1.4300e-02, -3.5297e-02,\n",
       "          -6.4087e-02,  4.5576e-02, -3.7363e-02, -4.2275e-01,  2.5096e-01,\n",
       "           5.1844e-02,  2.2476e-01, -2.3435e-01, -3.9022e-02,  1.4253e-01,\n",
       "           2.1255e-01, -2.2203e-01,  1.8360e-01, -3.0399e-01, -8.9777e-02,\n",
       "           1.0599e-01, -2.9458e-01, -3.7979e-01,  1.3672e-01,  3.4986e-01,\n",
       "           9.6259e-02, -1.7183e-02,  2.4675e-01, -9.0852e-02, -1.0352e-01,\n",
       "           1.7198e-01,  5.5760e-01,  2.8286e-02, -3.9149e-01, -2.6553e-01,\n",
       "           5.9615e-02, -1.0830e-01,  3.4901e-01,  1.0703e-01,  8.0337e-03,\n",
       "          -2.0425e-01, -1.4566e-01,  1.7463e-01,  8.6880e-02, -2.1251e-01,\n",
       "          -1.2433e-01,  1.8932e-01, -2.7032e-02, -1.6653e-01,  2.8663e-01,\n",
       "           1.9907e-01, -1.9429e-01, -7.7771e-02, -4.0573e-01,  4.1062e-02,\n",
       "           5.2726e-02,  2.3203e-01,  1.3462e-01,  1.8683e-01,  3.3088e-02,\n",
       "           2.0829e-01,  3.0594e-02, -4.5537e-02,  2.3520e-02, -2.6549e-01,\n",
       "          -2.2622e-01,  3.7735e-01,  6.6153e-02,  1.7802e-01, -2.2092e-01,\n",
       "           1.2381e-02,  2.1988e-01,  1.1947e-01,  1.3218e-01, -1.9301e-01,\n",
       "           1.8735e-01,  1.6142e-01,  1.4195e-01, -7.9677e-02,  1.7294e-01,\n",
       "          -4.9625e-02, -4.5984e-01,  3.0988e-01, -3.4637e-01, -1.2800e-01,\n",
       "          -3.5648e-02, -8.5142e-02, -2.2558e-01,  3.2591e-01, -3.7551e-02,\n",
       "           2.4137e-01,  4.1600e-02,  5.6149e-02,  1.3066e-02,  2.7907e-01,\n",
       "          -1.5393e-01,  1.0618e-01,  1.1082e-01,  1.5017e-01, -2.3065e-01,\n",
       "           1.5338e-01, -7.6308e-02,  1.7960e-01, -1.2916e-01, -3.3158e-01,\n",
       "           1.7021e-01,  2.2140e-01,  2.1501e-02,  1.3185e-01,  2.8122e-01,\n",
       "          -1.5739e-01, -3.4088e-01,  2.2483e-01,  6.9779e-01,  1.9859e-01,\n",
       "          -4.9550e-03,  2.1845e-01,  6.8135e-02,  8.4787e-02, -2.2717e-01,\n",
       "           1.0425e-01,  1.7590e-01, -1.3633e-01, -1.8604e-01, -8.6983e-02,\n",
       "          -7.4934e-02,  2.9343e-01, -1.0700e-01, -4.5940e-01, -1.9304e-01,\n",
       "           2.4277e-01,  5.4359e-02, -1.3501e-01,  9.1924e-02,  9.6067e-02,\n",
       "          -1.0987e-03, -1.8255e-01,  2.2746e-01, -1.1822e-01,  1.6849e-01,\n",
       "          -1.0865e-01, -1.4104e-01, -3.5742e-02, -2.3887e-02, -5.9210e-02,\n",
       "           4.3330e-01, -8.9599e-02,  2.2359e-01,  2.2501e-01,  8.8004e-02,\n",
       "          -4.6135e-02,  4.3944e-02, -2.7266e-01, -6.1376e-02,  2.4178e-01,\n",
       "           4.5800e-01, -3.5717e-01,  2.7629e-02,  5.1329e-02,  1.7484e-01,\n",
       "           3.1536e-02, -8.4335e-03,  2.1480e-01,  2.9372e-01,  2.0530e-01,\n",
       "           2.2041e-01, -1.3813e-01, -3.6940e-01, -1.8841e-02,  1.0176e-01,\n",
       "           2.1246e-01, -2.6879e-01, -2.9036e-01,  1.1975e-01,  2.3242e-02,\n",
       "          -3.5673e-02,  1.9585e-01,  6.0648e-02, -1.6483e-01, -3.9070e-01,\n",
       "           9.7001e-02, -9.7493e-02,  4.5577e-02, -2.1408e-01,  1.6976e-01,\n",
       "           4.0584e-02, -4.4453e-02,  2.8036e-01,  1.2745e-01,  1.6303e-01,\n",
       "           4.0992e-01,  2.7329e-01, -2.3385e-01, -4.0948e-02, -2.7886e-01,\n",
       "           2.3969e-01, -3.2360e-01,  3.1580e-02, -2.1599e-01, -2.7954e-01,\n",
       "          -1.0719e-01,  1.0058e-01,  1.5536e-01,  4.6504e-02, -3.0718e-02,\n",
       "          -1.2213e-01,  2.3354e-01, -7.4066e-02, -2.4623e-01, -6.4085e-03,\n",
       "           2.4696e-01,  2.3707e-02,  1.0861e-01,  7.2966e-02,  1.7058e-01,\n",
       "           1.1236e-02,  1.6971e-01, -2.3105e-02, -3.6808e-02, -3.6228e-01,\n",
       "           2.9764e-02,  9.7788e-02,  1.7754e-01,  2.0953e-01, -1.1902e-01,\n",
       "          -1.5664e-01],\n",
       "         [-3.3206e-01, -3.6978e-02, -1.3609e-01, -1.1886e-02,  2.7128e-02,\n",
       "          -2.3569e-01,  1.5029e-01, -3.4421e-01, -2.3518e-01, -5.7521e-02,\n",
       "          -2.4309e-01, -7.6265e-02, -1.8784e-01,  3.6008e-02,  8.5989e-02,\n",
       "          -1.3334e-01,  5.8636e-01,  4.8882e-02,  5.4389e-02,  1.0995e-01,\n",
       "          -2.7806e-01, -1.0848e-01,  3.1707e-02, -1.1361e-01,  4.1302e-02,\n",
       "           4.1659e-01, -4.3519e-01, -9.8073e-02, -7.1758e-03, -5.4940e-02,\n",
       "           1.7442e-01,  4.3722e-03,  1.0381e-01, -5.0606e-02, -2.5939e-01,\n",
       "          -6.9499e-02,  6.5190e-02,  9.9168e-03, -5.5296e-02,  2.3241e-01,\n",
       "           8.3277e-02,  3.0603e-01, -1.7156e-01, -1.3369e-01, -3.1305e-02,\n",
       "           1.3451e-01, -2.6695e-01,  2.5579e-01, -3.4255e-01,  1.8362e-01,\n",
       "           1.3206e-01, -2.5994e-01, -3.4365e-01,  4.6658e-02,  2.3811e-01,\n",
       "          -3.1035e-02,  1.5872e-01,  3.4903e-01, -5.9447e-02, -1.1206e-01,\n",
       "           1.2325e-01,  3.9764e-01,  3.0955e-02, -1.9596e-01, -2.2029e-01,\n",
       "          -1.0260e-01,  1.1379e-01,  4.3008e-01,  1.5224e-03, -1.3660e-01,\n",
       "           8.9697e-02, -1.1730e-01,  9.4323e-02, -5.5787e-02, -2.5315e-01,\n",
       "          -1.2697e-01,  2.6944e-01, -9.7573e-03, -1.9789e-01,  1.6849e-01,\n",
       "           2.8840e-01, -1.2322e-01, -8.3513e-03, -2.8003e-01,  4.7128e-03,\n",
       "           1.0765e-01,  1.1714e-01,  3.8798e-01,  1.0847e-01, -1.5451e-02,\n",
       "           6.4849e-02,  4.6773e-02, -3.5347e-02, -1.2734e-01, -1.5335e-01,\n",
       "          -2.7659e-01,  6.8977e-01,  1.4759e-01, -1.4061e-01, -4.0227e-01,\n",
       "          -6.0221e-02,  7.0012e-03,  2.1879e-01,  1.3537e-01, -2.2463e-01,\n",
       "          -1.4754e-01,  2.5513e-01,  3.9127e-01,  1.0379e-01,  1.1540e-01,\n",
       "          -5.0773e-02, -4.7797e-01,  2.0765e-01, -2.6147e-01, -2.3409e-02,\n",
       "          -4.3356e-02, -1.2324e-01, -1.4349e-01,  2.1770e-01,  2.5833e-02,\n",
       "           2.0172e-01, -2.1814e-01, -3.5556e-02,  1.6547e-02,  1.6732e-01,\n",
       "          -1.3435e-01,  2.1435e-01,  5.8985e-03, -5.8466e-02, -5.1170e-02,\n",
       "           2.8950e-01, -1.2720e-01,  1.8832e-01, -2.1043e-01, -2.7790e-01,\n",
       "           9.5578e-02,  1.0426e-01, -2.8347e-02,  8.0608e-02,  3.9543e-01,\n",
       "           1.5300e-03, -3.8060e-01, -2.9065e-02,  5.1594e-01,  1.3683e-01,\n",
       "          -3.0722e-01,  1.4513e-01, -1.7696e-02,  8.6038e-02, -3.2518e-01,\n",
       "           1.4448e-01, -1.0705e-01,  8.2740e-02, -1.5980e-01, -7.8058e-02,\n",
       "           1.1547e-01,  4.3019e-01, -1.3094e-01, -4.5696e-01, -2.3267e-01,\n",
       "           6.4099e-02,  4.0425e-02, -1.6890e-01,  1.0422e-01, -5.3939e-02,\n",
       "          -5.6646e-02,  9.2590e-02,  3.0514e-01,  4.9479e-02, -2.2098e-01,\n",
       "          -1.2899e-02, -2.1086e-01,  1.1041e-01,  5.5987e-02, -4.2275e-02,\n",
       "           1.7879e-01,  2.3144e-02,  7.2323e-02,  2.5311e-01,  3.0157e-01,\n",
       "           1.0155e-01,  3.9935e-02,  3.7530e-02,  1.6337e-01,  6.1134e-03,\n",
       "           3.9612e-01, -6.5135e-01, -1.9760e-01,  6.2382e-02,  1.2972e-01,\n",
       "          -9.8955e-02,  1.2321e-02,  4.9358e-01,  1.6408e-01,  9.8901e-02,\n",
       "           2.1644e-01,  2.1009e-02, -3.7713e-01,  1.1038e-01, -6.5217e-02,\n",
       "          -6.6215e-02, -1.8050e-01, -3.3451e-01,  3.1439e-02, -1.5654e-01,\n",
       "           5.2648e-02,  2.2228e-01,  1.1617e-01, -1.8774e-01, -4.4554e-01,\n",
       "           1.4687e-01, -1.3115e-01,  1.8265e-02, -2.0848e-01,  3.7663e-01,\n",
       "          -2.7450e-03,  1.9069e-01,  2.8170e-01, -2.7046e-02,  1.2702e-01,\n",
       "           3.2874e-01,  2.0629e-01, -2.3875e-01, -9.5440e-02, -1.9715e-01,\n",
       "           1.4103e-01, -7.9589e-02,  4.7623e-02, -2.2474e-01, -1.8016e-01,\n",
       "           7.9543e-02,  8.9654e-02,  1.7468e-01,  6.1277e-03, -6.2640e-03,\n",
       "          -1.9294e-01, -8.9674e-02, -2.0598e-01, -2.1590e-01, -1.5393e-02,\n",
       "          -2.7269e-02, -9.7530e-02,  2.3535e-01, -1.0302e-01,  1.2370e-01,\n",
       "           3.1219e-02,  5.8274e-02,  2.6105e-02, -2.7731e-01, -1.5240e-01,\n",
       "          -1.4619e-02,  2.0936e-01, -8.7771e-03,  1.2937e-01,  1.0747e-01,\n",
       "           1.9813e-02],\n",
       "         [-5.0023e-01, -4.9551e-02,  7.4404e-02, -1.3691e-01, -2.3388e-01,\n",
       "          -3.4497e-01,  8.9983e-03, -2.6054e-01, -1.7425e-01, -1.8383e-01,\n",
       "          -1.7024e-01, -1.1800e-01, -1.9868e-01, -1.1145e-01, -1.8636e-02,\n",
       "          -2.3380e-01,  7.1066e-01,  7.0373e-02,  2.0250e-01,  8.9822e-02,\n",
       "          -2.8754e-01, -1.8149e-01,  1.4842e-02,  1.6265e-01,  1.8856e-01,\n",
       "           5.3383e-01, -5.2482e-01, -9.8644e-02, -1.4063e-02, -1.4113e-01,\n",
       "           2.2650e-01,  2.8113e-02, -8.3636e-02, -8.2725e-02, -1.7845e-01,\n",
       "           1.1261e-01,  9.3509e-03,  8.3837e-03, -2.1577e-01,  2.6008e-01,\n",
       "          -3.0012e-02,  2.9653e-01, -2.4012e-01,  2.2486e-02,  2.1379e-01,\n",
       "           1.1313e-01, -2.6404e-01,  2.3370e-01, -1.7346e-01, -1.5639e-01,\n",
       "           2.6362e-02, -2.6446e-01, -4.8570e-01,  2.2596e-01,  3.2600e-01,\n",
       "          -1.2090e-02,  1.9744e-01,  2.5436e-01,  8.1253e-03, -2.7358e-01,\n",
       "           1.1509e-01,  5.8261e-01,  7.0052e-02, -4.5684e-01, -2.2542e-01,\n",
       "          -8.7304e-02, -2.3743e-01,  2.6652e-01,  9.0167e-02, -1.0174e-01,\n",
       "           1.0897e-01, -1.2788e-01, -8.7842e-02, -1.8605e-02, -1.2420e-01,\n",
       "          -5.2369e-02,  1.6393e-01,  2.9418e-02, -1.0737e-01,  3.8263e-01,\n",
       "          -1.7111e-02, -6.0640e-02, -2.0559e-01, -2.5332e-01,  7.8463e-03,\n",
       "          -7.8470e-02,  2.3640e-01, -1.1619e-01,  1.8831e-01,  4.5927e-02,\n",
       "          -4.1447e-02, -5.7736e-02, -7.5438e-03,  1.0255e-02, -2.3646e-01,\n",
       "          -2.9879e-01,  3.6207e-01,  8.8688e-02,  2.8303e-02, -3.7289e-01,\n",
       "           8.4224e-02,  2.8057e-01,  3.8571e-02,  2.5029e-01, -2.1685e-01,\n",
       "           7.9415e-03,  1.5846e-01,  3.2295e-01,  1.3027e-01,  2.0736e-01,\n",
       "          -9.7658e-02, -3.6134e-01,  1.3955e-01, -1.1069e-01, -9.7323e-02,\n",
       "          -1.3209e-01, -5.0442e-04, -3.1822e-01,  5.0382e-01,  2.4678e-02,\n",
       "           2.9983e-02, -4.9965e-02, -3.7123e-02,  4.2578e-02,  3.2248e-01,\n",
       "          -1.8029e-01,  1.2981e-01,  6.0230e-02,  1.8107e-01, -1.4513e-01,\n",
       "           1.9513e-01, -1.3715e-01,  1.8979e-01, -7.0441e-02, -3.0356e-01,\n",
       "           4.9095e-02,  3.8738e-01,  7.5452e-02,  1.8521e-01,  3.0940e-01,\n",
       "          -1.8955e-01, -1.8247e-01,  9.8850e-02,  4.5411e-01, -7.9998e-02,\n",
       "          -1.7948e-01,  3.3522e-01,  1.4192e-01,  2.4410e-01, -1.8838e-01,\n",
       "           2.6500e-01,  6.0869e-02, -9.1607e-02, -1.2315e-01, -2.9779e-02,\n",
       "          -2.9900e-02,  5.0566e-01, -2.5680e-01, -3.9030e-01, -9.2475e-02,\n",
       "           1.6046e-01, -8.9681e-02, -2.1900e-02, -2.4349e-02,  7.7380e-02,\n",
       "           1.3468e-01, -1.6722e-01,  3.6649e-01, -1.6845e-01, -1.0524e-01,\n",
       "          -1.5482e-01, -2.5388e-01,  1.7262e-01, -8.4470e-02,  1.6841e-01,\n",
       "           4.0846e-01, -2.4072e-01,  1.8560e-01,  2.2741e-01,  2.9660e-01,\n",
       "          -1.9487e-01,  9.0245e-02, -9.1061e-02, -5.6734e-03,  6.9556e-02,\n",
       "           4.9174e-01, -2.7505e-01, -7.7851e-02, -7.4126e-02,  1.5098e-01,\n",
       "          -2.1134e-02,  1.2871e-01,  3.9024e-01,  2.7223e-01,  1.2860e-01,\n",
       "           2.4533e-01, -3.9960e-02, -4.3281e-01,  1.8524e-01, -7.3505e-03,\n",
       "           6.1750e-02, -1.4354e-01, -1.7832e-01,  1.6421e-01,  2.0101e-01,\n",
       "           1.8906e-01,  1.2358e-01, -8.0727e-02, -2.4135e-01, -3.5486e-01,\n",
       "           7.2831e-02, -2.5294e-01,  5.7668e-02, -2.6772e-01,  2.9191e-01,\n",
       "           4.7724e-02,  9.6398e-02,  3.0032e-01,  1.4923e-01,  2.9997e-01,\n",
       "           4.7386e-01,  3.0658e-01, -2.7668e-01,  9.5160e-03, -2.4171e-01,\n",
       "           1.0997e-01, -2.4977e-01,  1.6108e-02, -3.4104e-01, -2.7646e-01,\n",
       "          -9.8287e-02,  1.1835e-01,  2.2008e-01,  2.6456e-01, -4.5095e-02,\n",
       "          -9.1824e-02,  1.6288e-01,  2.5202e-02, -2.2758e-01,  8.7797e-02,\n",
       "           6.5618e-02, -2.0743e-02,  1.1566e-01,  1.2115e-01,  3.6372e-01,\n",
       "          -4.6478e-02,  2.4579e-01, -1.9224e-02, -2.0628e-01, -9.6051e-02,\n",
       "          -5.9889e-02,  1.2839e-01,  1.8408e-01,  3.0164e-01,  1.4698e-01,\n",
       "           1.1420e-01]]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=512, \n",
    "    d_model=256\n",
    ")\n",
    "\n",
    "att_layer = TransformerAttention(\n",
    "    d_model=256, \n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "x = enc(torch.tensor([1, 2, 3], device=dev).unsqueeze(0))\n",
    "att_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065acf4",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025690",
   "metadata": {},
   "source": [
    "- According, to section 3.3 of the paper, this has 2 layers\n",
    "- d_model -> d_ff -> d_model\n",
    "- same parameters for every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4cdf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_ff,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=self.d_ff,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.Tensor):\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "        f2 = self.fc2(f1)\n",
    "        return f2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d124573",
   "metadata": {},
   "source": [
    "## The Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24f19136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int, \n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.att_layer = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=dev)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_att = self.att_layer(x)\n",
    "\n",
    "        x_att = self.dropout(x_att)\n",
    "        x_norm1 = self.norm1(x + x_att)\n",
    "\n",
    "        x_ff = self.ffn(x_norm1)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm2 = self.norm2(x_ff + x_norm1)\n",
    "        \n",
    "        return x_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c205fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    d_model= 512,\n",
    "    d_ff=2048,\n",
    "    num_heads=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "756e27b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (att_layer): TransformerAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): FeedForwardNetwork(\n",
       "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "encoder(\n",
    "    enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0))\n",
    "    )\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538db08",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf6438",
   "metadata": {},
   "source": [
    "only the final encoder output is used for all the decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78af3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.att_layer1 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.att_layer2 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff\n",
    "        )\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=dev), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, value=float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                cross_input:torch.Tensor,\n",
    "                padding_mask:torch.Tensor = None\n",
    "                ):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        causal_mask = self.create_causal_mask(seq_len=seq_length)\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1) #unsqeeze the mast for self attention\n",
    "\n",
    "        x_att1 = self.att_layer1( #self-attention\n",
    "            seq=x,\n",
    "            att_mask = causal_mask\n",
    "        )\n",
    "\n",
    "        x_att1 = self.dropout(x_att1)\n",
    "        x_norm1 = self.norm1(x_att1 + x)\n",
    "\n",
    "        x_att2 = self.att_layer2( #cross attention\n",
    "            seq=x_norm1,\n",
    "            key_value_states=cross_input,\n",
    "            att_mask = padding_mask\n",
    "        )\n",
    "\n",
    "        x_att2 = self.dropout(x_att2)\n",
    "        x_norm2 = self.norm2(x_att2)\n",
    "\n",
    "        x_ff = self.ffn(x_norm2)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm3 = self.norm3(x_ff)\n",
    "        \n",
    "        return x_norm3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "39f58b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5318, -0.0829, -0.0234,  ..., -0.5824, -1.5428,  0.5702],\n",
       "         [-0.1794,  1.1398,  0.2151,  ...,  0.1587, -0.6725,  0.7063],\n",
       "         [-0.3512,  1.3743,  0.6984,  ...,  0.2048, -0.0566,  1.0124]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "decoder = TransformerDecoder(\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "x = encoder(\n",
    "    enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0))\n",
    "    )\n",
    "\n",
    "decoder.forward(enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0)), x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb7bdf",
   "metadata": {},
   "source": [
    "## Transformer Encoder and Decoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bce69e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
