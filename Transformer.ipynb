{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39faeef",
   "metadata": {},
   "source": [
    "# Implementing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c19c11f",
   "metadata": {},
   "source": [
    "Reference: [Implementation_Tutorial](Transformer_Implementation_Tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b255351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from math import log, sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7dbb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaf091",
   "metadata": {},
   "source": [
    "## Embdedding and Position Encoding Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5692a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWithPositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, \n",
    "                 d_embed: int, \n",
    "                 d_model: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=d_embed,\n",
    "            device=dev\n",
    "        )\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=d_embed,\n",
    "            out_features=d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        self.scaling = float(sqrt(self.d_model))\n",
    "\n",
    "        self.layerNorm = nn.LayerNorm(\n",
    "            self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    @staticmethod # decorator that indicates that the following function doesn't operate on `self`\n",
    "    def create_positional_encoding(seq_length:int, \n",
    "                                   d_model:int, \n",
    "                                   batch_size:int\n",
    "                                   ):\n",
    "\n",
    "        positions = torch.arange(seq_length, dtype=torch.long, device=dev)\\\n",
    "            .unsqueeze(1) # shape (seq_length, 1) i.e. makes it vertical\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            (torch.arange(0, d_model, 2)/d_model)*(-4)*log(10)\n",
    "        ).to(dev)\n",
    "        \n",
    "        pe = torch.zeros(size=(seq_length, d_model), dtype=torch.long, device=dev) # the tensor to be multiplied to positions tensor to get pe\n",
    "        pe[:, 0::2] = torch.sin(positions*div_term) # for even dimensions\n",
    "        pe[:, 1::2] = torch.cos(positions*div_term) # for odd dimensions\n",
    "        pe = pe.unsqueeze(0).expand(batch_size, -1, -1) # copy out the encodings for each batch\n",
    "        return pe\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "\n",
    "        # step 1: make embeddings\n",
    "        token_embedding = self.embedding(x)\n",
    "\n",
    "        # step 2: go from d_embed to d_model\n",
    "        token_embedding = self.projection(token_embedding) \\\n",
    "            * self.scaling # multiplying with scaling factor, just like in the paper\n",
    "\n",
    "        # step 3: add positional encoding\n",
    "        pos_encoding = self.create_positional_encoding(\n",
    "            seq_length=seq_length, \n",
    "            d_model = self.d_model,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        #step 4: normalize the sum of pos encoding and token_embed\n",
    "        norm_sum = self.layerNorm(pos_encoding + token_embedding)\n",
    "        op = self.dropout(norm_sum)\n",
    "        return op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eafd9ac",
   "metadata": {},
   "source": [
    "## Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1a4ce",
   "metadata": {},
   "source": [
    "- Two types of attention I learnt:\n",
    "  - **Self-Attention:** key values come from the same input tensor\n",
    "  - **Cross-Attention:** key values come fromt he output of a different multi-head attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226fb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 dropout_p: float = 0.1\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if (d_model % num_heads) != 0: raise ValueError(f'`d_model` not divisible by `num_heads`')\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_heads = self.d_model // self.num_heads\n",
    "        self.scale_factor = float(1.0 / sqrt(self.d_heads))\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "        #linear transformations\n",
    "        self.q_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.k_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.v_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "    def forward(self, \n",
    "                seq: torch.Tensor, \n",
    "                key_value_states:torch.Tensor = None, \n",
    "                att_mask: torch.Tensor = None):\n",
    "        batch_size, seq_length, d_model = seq.size()\n",
    "\n",
    "        Q_state: torch.Tensor = self.q_proj(seq)\n",
    "        if key_value_states is not None:\n",
    "            kv_seq_len = key_value_states.size(1)\n",
    "            K_state: torch.Tensor = self.k_proj(key_value_states)\n",
    "            V_state: torch.Tensor = self.v_proj(key_value_states)\n",
    "        else:\n",
    "            kv_seq_len = seq_length\n",
    "            K_state: torch.Tensor = self.k_proj(seq)\n",
    "            V_state: torch.Tensor = self.v_proj(seq)\n",
    "\n",
    "        Q_state = Q_state.view(batch_size, seq_length, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        K_state = K_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "        V_state = V_state.view(batch_size, kv_seq_len, self.num_heads, self.d_heads).transpose(1, 2)\n",
    "\n",
    "        Q_state = Q_state * self.scale_factor\n",
    "        \n",
    "        self.att_matrix = torch.matmul(Q_state, K_state.transpose(-1, -2))\n",
    "        \n",
    "\n",
    "        if att_mask is not None:\n",
    "            self.att_matrix = self.att_matrix + att_mask # yes, in this case the mask is not multiplied, but added. This is to ensure that after softmax the things to be excluded are 0\n",
    "        \n",
    "        att_score = F.softmax(self.att_matrix, dim=-1) # torch.nn.Softmax() is used in __init__, F.softmax() is used for these inline operations.\n",
    "        att_score = self.dropout(att_score)\n",
    "        att_op = torch.matmul(att_score, V_state)\n",
    "\n",
    "        #concatenating all heads \n",
    "        att_op = att_op.transpose(1, 2)\n",
    "        att_op = att_op.contiguous().view(batch_size, seq_length, self.num_heads*self.d_heads)\n",
    "\n",
    "        att_op = self.output_proj(att_op)\n",
    "\n",
    "        return att_op\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f75cd",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e6bedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-5.6083e-02, -2.7584e-01,  1.2233e-01,  1.0724e-01,  1.3438e-01,\n",
       "           5.8057e-02, -1.5747e-01,  6.2093e-02, -1.0438e-01,  1.4907e-01,\n",
       "           1.0775e-01, -1.8889e-01, -4.2672e-01, -2.6931e-01,  1.7088e-01,\n",
       "           9.8167e-02, -2.5291e-01,  6.9719e-02,  2.9859e-01, -4.9563e-02,\n",
       "          -9.1499e-02,  6.5453e-02,  3.4010e-01,  2.3936e-01,  4.1414e-01,\n",
       "          -2.1820e-01, -1.4620e-01, -1.7827e-01, -1.0829e-01,  1.2271e-01,\n",
       "           6.5892e-02,  9.5665e-02, -3.1794e-01, -4.1636e-02,  9.9729e-02,\n",
       "          -4.3897e-01, -2.3396e-01,  1.1752e-01, -1.2415e-01,  1.5225e-01,\n",
       "           1.6180e-01,  2.2708e-01,  1.4612e-01, -4.0081e-01, -2.2000e-01,\n",
       "           1.8675e-01,  2.6911e-01, -6.4734e-02, -1.2841e-01,  9.5680e-02,\n",
       "          -3.5189e-01,  1.7776e-01,  1.4775e-01, -2.2859e-01,  1.3291e-01,\n",
       "          -3.9022e-01,  1.0306e-01, -1.6720e-01,  9.4699e-02,  1.5673e-01,\n",
       "          -2.1388e-01,  1.0639e-01,  1.4410e-01, -8.3046e-02, -4.2238e-02,\n",
       "           1.6983e-01, -1.2171e-01,  1.2972e-01, -5.6353e-02, -1.5727e-02,\n",
       "          -2.5696e-01,  1.2624e-02,  3.4317e-01,  1.9382e-01, -2.2854e-01,\n",
       "          -6.6539e-02, -1.0283e-01, -1.7857e-01, -1.1238e-01,  1.1208e-01,\n",
       "           5.8125e-02, -2.0553e-01,  2.3658e-01, -1.6149e-01, -6.2530e-02,\n",
       "           4.2639e-01, -4.3898e-01,  3.3417e-01,  3.7405e-02,  5.9797e-01,\n",
       "          -1.7855e-01,  4.0749e-01, -9.5768e-02, -8.9469e-02, -9.6540e-02,\n",
       "          -1.9268e-01, -2.9595e-01,  2.3800e-02, -1.5166e-01, -2.4729e-01,\n",
       "          -3.2505e-01, -5.7000e-02, -1.0925e-01,  4.0832e-02, -1.1674e-01,\n",
       "          -1.7799e-01,  5.9562e-02, -2.6263e-01, -3.0517e-01, -1.2947e-01,\n",
       "          -1.1983e-01,  2.1627e-02,  2.0462e-01,  6.4131e-02,  5.0938e-02,\n",
       "           2.0321e-01, -2.1723e-01,  1.0884e-02,  1.0894e-01,  5.0504e-02,\n",
       "          -1.5845e-01,  8.2792e-02,  4.3232e-02,  3.6855e-01, -1.6088e-01,\n",
       "          -6.4417e-02, -2.1000e-01, -2.1966e-01,  3.9716e-03, -3.0590e-01,\n",
       "           7.3408e-02, -8.0903e-02, -1.5993e-02,  7.6241e-02, -4.3200e-02,\n",
       "           6.1310e-02,  4.4567e-01,  1.0891e-01,  8.6578e-03,  2.3373e-01,\n",
       "           1.2218e-01,  1.8764e-01, -1.1873e-01,  1.6454e-01,  9.7655e-03,\n",
       "          -1.7530e-01, -3.8253e-01,  2.0054e-01, -3.0663e-01, -2.8612e-01,\n",
       "          -4.4269e-02,  9.6697e-02, -1.4034e-01, -3.6336e-01,  4.9577e-02,\n",
       "          -4.1873e-01, -5.4234e-02, -5.1868e-02, -1.6114e-01,  2.1025e-01,\n",
       "          -1.6637e-01, -3.0660e-02, -3.7103e-01,  2.2234e-01, -1.7200e-01,\n",
       "          -1.0494e-01,  2.3831e-01, -1.6802e-01,  4.0005e-01,  7.1695e-02,\n",
       "          -1.2294e-01, -4.5259e-01, -4.6235e-01,  8.8425e-02,  1.4564e-01,\n",
       "          -2.4396e-01,  7.7057e-02, -2.2771e-01,  1.9804e-02, -3.7360e-02,\n",
       "          -1.8393e-01, -4.2851e-01, -2.1551e-01, -2.9559e-01,  1.0575e-01,\n",
       "          -2.1512e-01, -8.8692e-03, -1.0564e-01,  4.7740e-01, -3.8438e-01,\n",
       "           2.4968e-01, -3.7861e-01,  1.8455e-01, -1.6720e-01, -3.3035e-03,\n",
       "          -1.5955e-01,  4.3418e-01, -1.5944e-01, -2.2141e-01, -3.2255e-02,\n",
       "           3.6094e-02,  3.3106e-01,  2.1529e-01, -5.5760e-02,  1.2515e-01,\n",
       "          -1.6073e-01,  2.1761e-01,  2.6841e-01, -1.0189e-01, -3.5115e-01,\n",
       "          -1.0825e-02,  1.2758e-01,  2.0153e-02,  5.2860e-02,  3.6571e-01,\n",
       "           5.0425e-01,  2.9738e-01,  1.2951e-01,  1.5241e-01,  4.9241e-01,\n",
       "           1.0216e-01,  2.8045e-02,  2.1204e-01,  3.8261e-01,  2.9467e-01,\n",
       "           3.5479e-01,  3.5023e-01, -6.1073e-02,  5.3885e-02,  9.4178e-02,\n",
       "          -1.3700e-01, -2.5802e-01,  3.3762e-01, -1.1885e-02, -3.7677e-02,\n",
       "          -2.2019e-01,  3.5810e-03, -4.0987e-02,  1.3382e-01, -4.4025e-01,\n",
       "          -3.1217e-01,  3.3199e-01, -2.1835e-01, -2.1106e-01, -1.3786e-01,\n",
       "          -1.5483e-01,  2.2993e-01,  3.7165e-01, -2.2554e-01,  4.1299e-01,\n",
       "           1.5276e-01,  3.1614e-01,  9.5291e-02, -2.8651e-01,  1.1058e-01,\n",
       "           4.5896e-02],\n",
       "         [-3.4020e-02, -6.0853e-02,  1.1526e-01,  6.2423e-02,  1.7434e-01,\n",
       "           2.0097e-02, -1.6766e-01, -7.0699e-02,  1.4381e-02,  3.5610e-01,\n",
       "           1.6827e-01, -9.9572e-02, -3.4199e-01, -3.4691e-01,  1.2840e-01,\n",
       "          -2.7601e-02,  6.4792e-02,  1.6598e-01,  1.5090e-01, -1.8735e-02,\n",
       "          -1.5691e-01, -6.2008e-02,  1.0993e-01,  1.2286e-01,  4.1051e-01,\n",
       "          -1.6505e-01, -1.2455e-01, -2.8263e-01, -1.9149e-01,  4.1955e-02,\n",
       "           8.0851e-02, -2.8030e-03, -3.0833e-01, -2.5201e-02,  2.3716e-01,\n",
       "          -3.0430e-01, -3.2776e-01,  1.2867e-01,  2.9500e-02,  2.6928e-01,\n",
       "           4.6300e-02,  3.7416e-01,  1.2398e-01, -3.7512e-01, -2.2418e-01,\n",
       "           8.5544e-02,  4.0262e-01, -6.9198e-02, -1.9088e-01,  5.0297e-02,\n",
       "          -4.3528e-01,  1.2532e-01,  1.5906e-01, -1.6147e-01,  1.3676e-01,\n",
       "          -3.2771e-01,  7.8249e-02, -1.8530e-01,  1.7073e-01,  2.3994e-01,\n",
       "          -2.7204e-01,  1.2472e-01,  1.8839e-01,  3.7668e-02, -1.9685e-01,\n",
       "           1.9226e-01, -1.3639e-01, -6.1410e-02,  9.6189e-02,  5.1983e-02,\n",
       "          -2.7491e-01, -1.4679e-01,  3.5553e-01,  2.4404e-01, -2.4394e-01,\n",
       "          -1.1047e-02,  2.1063e-02, -1.4307e-02,  1.2479e-01, -6.6148e-03,\n",
       "          -9.2956e-02, -2.7996e-01,  2.5661e-01, -5.6885e-02,  4.6517e-02,\n",
       "           3.6698e-01, -3.5895e-01,  3.6166e-01, -1.1604e-02,  4.7683e-01,\n",
       "          -4.6108e-02,  4.5124e-01, -8.8968e-02, -1.8528e-01, -5.1003e-02,\n",
       "          -1.1881e-01, -1.3651e-01, -9.5623e-02, -2.0631e-01, -2.3413e-01,\n",
       "          -3.5127e-01, -9.5082e-02, -2.3061e-03, -6.5786e-03, -1.0216e-01,\n",
       "          -1.2965e-01, -9.2774e-02, -1.4088e-01, -5.1381e-01, -1.7109e-01,\n",
       "          -2.8362e-02, -2.6051e-02,  2.0279e-01, -4.5080e-02,  9.4591e-02,\n",
       "           8.6281e-02, -2.7727e-01, -6.2088e-02,  1.4226e-01,  5.3039e-02,\n",
       "          -1.9193e-01,  1.0203e-01,  1.5843e-01,  2.8596e-01, -1.4275e-01,\n",
       "          -1.2524e-01, -1.0855e-01, -2.0745e-01,  1.3363e-01, -4.1004e-01,\n",
       "           6.5293e-02, -1.7557e-01,  5.6919e-02,  2.9382e-02, -1.4879e-01,\n",
       "           1.3227e-01,  3.9705e-01, -2.5468e-02,  1.2865e-01,  2.3580e-01,\n",
       "           1.6965e-01,  5.4638e-02, -3.3604e-01,  1.0047e-01, -2.1659e-03,\n",
       "          -4.1861e-02, -4.4917e-01,  1.5140e-01, -2.0305e-01, -5.9529e-02,\n",
       "           5.7733e-02,  1.1940e-01, -9.0329e-02, -3.7783e-01,  2.1219e-01,\n",
       "          -3.6762e-01, -1.9755e-01, -6.3177e-03, -7.4598e-02,  7.6739e-02,\n",
       "          -1.3913e-01, -7.4281e-02, -2.9036e-01,  3.3878e-01, -4.7397e-02,\n",
       "          -1.0864e-01,  1.6070e-01, -2.9102e-01,  3.3178e-01,  5.4992e-02,\n",
       "          -2.3644e-02, -5.0549e-01, -4.8889e-01,  1.1644e-01,  1.3731e-01,\n",
       "          -5.0707e-02,  2.3093e-01, -3.3478e-01, -8.4538e-02,  1.6956e-02,\n",
       "           1.1514e-02, -4.0255e-01,  3.4488e-02, -1.7180e-01,  1.8246e-01,\n",
       "          -9.1891e-02,  8.5648e-02, -1.3568e-01,  3.4687e-01, -3.5086e-01,\n",
       "           1.9733e-01, -1.9264e-01,  2.3955e-01, -1.1149e-01, -2.6870e-02,\n",
       "          -2.8500e-01,  3.8188e-01, -1.3485e-01, -1.8108e-01, -9.2435e-02,\n",
       "           9.2127e-02,  3.0057e-01,  4.9344e-01, -8.8380e-02,  1.0351e-02,\n",
       "          -1.4632e-01,  3.0249e-01,  2.7729e-01, -4.3490e-02, -3.1453e-01,\n",
       "          -1.7774e-01,  2.0030e-01,  7.7894e-02,  1.2416e-01,  3.4413e-01,\n",
       "           3.8811e-01,  2.3651e-01, -1.8910e-01,  1.5704e-01,  4.5054e-01,\n",
       "           2.0460e-01,  1.3327e-01,  3.0744e-01,  4.7411e-01,  2.9564e-01,\n",
       "           5.0837e-01,  2.1494e-01, -1.8668e-01,  7.8567e-02,  2.7406e-01,\n",
       "           6.7392e-02, -2.7703e-01,  3.1393e-01,  2.2063e-01,  5.2291e-03,\n",
       "          -1.1667e-01,  3.1455e-02, -1.3440e-02,  1.4109e-01, -4.2207e-01,\n",
       "          -2.9148e-01,  3.1015e-01, -1.7691e-01, -4.9940e-02, -1.0808e-01,\n",
       "          -1.3738e-01,  2.2528e-01,  3.7139e-01, -2.3538e-01,  4.6794e-01,\n",
       "          -4.0709e-02,  4.6508e-01,  2.4183e-01, -2.3682e-01,  1.9440e-01,\n",
       "          -9.4425e-03],\n",
       "         [-7.0374e-02, -3.1816e-01,  5.3554e-02,  2.4693e-01,  3.4338e-01,\n",
       "           1.2372e-01, -5.7628e-02,  1.0343e-03, -1.8635e-01,  1.4965e-01,\n",
       "           2.3981e-01, -3.2809e-01, -3.6037e-01, -2.0741e-01,  1.6996e-01,\n",
       "           5.1782e-02, -3.1496e-02,  1.6108e-01,  2.6570e-01, -4.4812e-02,\n",
       "          -3.9916e-02, -2.3109e-02,  1.1899e-01,  1.3457e-01,  4.2724e-01,\n",
       "          -1.7602e-01,  8.9763e-03, -2.5651e-01, -1.5406e-01,  4.2812e-02,\n",
       "          -4.5999e-02,  5.8409e-02, -2.8341e-01,  8.7913e-02,  7.1368e-02,\n",
       "          -2.9728e-01, -3.2635e-01,  5.8610e-02, -4.3606e-02,  2.3485e-01,\n",
       "           7.3432e-03,  3.1672e-01, -1.6041e-02, -4.7819e-01, -1.1455e-01,\n",
       "           1.9120e-01,  8.3664e-02, -1.3182e-02, -1.0624e-01,  6.1198e-02,\n",
       "          -3.9313e-01,  9.4110e-02, -3.5348e-02, -3.4698e-01,  2.0232e-01,\n",
       "          -3.9307e-01,  1.9636e-01, -2.9482e-01,  4.8369e-02,  9.4474e-02,\n",
       "          -2.0338e-01, -3.4120e-02,  8.7486e-02, -1.2845e-02,  7.1889e-03,\n",
       "           2.3841e-01, -2.7377e-01,  1.0193e-01, -9.5629e-03, -1.7181e-01,\n",
       "          -3.4632e-01, -1.3818e-01,  3.5831e-01,  2.2000e-01, -4.2866e-01,\n",
       "          -3.1231e-03, -3.6485e-02, -1.1001e-01, -1.0819e-01, -4.8015e-02,\n",
       "          -1.5730e-01, -1.7518e-01,  2.3163e-01, -3.0347e-01, -1.2696e-01,\n",
       "           4.6255e-01, -5.6579e-01,  4.5220e-01, -5.7574e-02,  7.3236e-01,\n",
       "          -2.2152e-01,  5.1094e-01,  3.7091e-02, -1.2152e-01, -1.4096e-01,\n",
       "          -1.2728e-01, -7.5829e-02,  1.3631e-03, -2.4200e-01, -1.3882e-01,\n",
       "          -4.0385e-01, -2.1819e-01, -1.8556e-01, -7.2709e-03,  6.1179e-02,\n",
       "          -2.1829e-01, -5.7891e-02, -2.5918e-01, -3.9788e-01, -1.1093e-01,\n",
       "          -5.6720e-03,  1.3802e-01,  2.7278e-01,  2.4059e-02, -5.8657e-02,\n",
       "           2.5019e-01, -1.3231e-01, -2.0686e-02,  1.1586e-01,  7.9880e-02,\n",
       "          -1.0191e-01,  8.6534e-02, -4.8572e-02,  5.4520e-01, -2.6349e-01,\n",
       "           8.4288e-02, -6.8919e-02, -2.8219e-01,  1.1003e-01, -3.2456e-01,\n",
       "           3.9649e-02,  1.1855e-02, -1.2634e-01,  1.1912e-01, -6.8578e-02,\n",
       "           8.1141e-02,  5.3796e-01, -1.5240e-01,  1.0512e-01,  8.0737e-02,\n",
       "           2.4522e-01,  3.0445e-02, -1.0645e-01,  1.1296e-01, -7.2646e-04,\n",
       "          -3.1606e-01, -4.4660e-01,  2.0520e-01, -2.5471e-01, -2.7307e-01,\n",
       "           1.9652e-02,  4.6285e-02,  4.1644e-02, -4.8222e-01,  1.8927e-01,\n",
       "          -3.1900e-01, -3.2138e-01, -7.8165e-02,  8.6658e-02,  2.7492e-01,\n",
       "          -1.4388e-01, -5.7785e-02, -2.9339e-01,  2.8971e-01, -1.4068e-01,\n",
       "          -1.1824e-01,  3.2488e-01, -3.9776e-01,  4.8138e-01,  4.3342e-02,\n",
       "          -1.8764e-01, -4.0442e-01, -3.5252e-01,  7.5968e-02,  2.3124e-01,\n",
       "          -3.4728e-01,  2.2046e-01, -1.6552e-01,  4.8412e-03, -1.0745e-01,\n",
       "          -6.0727e-02, -4.1017e-01,  3.8037e-02, -3.6589e-01,  1.0583e-01,\n",
       "          -3.4121e-01,  9.6246e-02, -1.8832e-01,  6.4455e-01, -2.6516e-01,\n",
       "           2.7203e-01, -2.3304e-01,  2.6095e-01, -7.0116e-02, -7.1665e-02,\n",
       "          -2.4414e-01,  4.1051e-01, -1.1964e-01, -2.9881e-01, -1.0191e-01,\n",
       "           4.3922e-02,  2.1221e-01,  3.6120e-01, -1.2785e-01,  1.4273e-01,\n",
       "          -7.2641e-02,  3.4066e-01,  2.1192e-01, -1.3820e-01, -3.9792e-01,\n",
       "          -1.9620e-01,  3.0962e-01,  5.7098e-03, -3.8163e-02,  5.2448e-01,\n",
       "           5.7526e-01,  2.0369e-01,  2.5097e-01,  2.8812e-01,  4.8819e-01,\n",
       "           4.5454e-02,  6.2928e-02,  4.2286e-01,  5.2564e-01,  3.6192e-01,\n",
       "           4.8923e-01,  3.0728e-01, -1.5251e-01,  1.4831e-01,  4.6806e-02,\n",
       "          -7.5821e-03, -3.3958e-01,  3.1284e-01,  7.4576e-02, -1.1637e-01,\n",
       "          -1.3684e-01,  2.5137e-02,  4.0699e-02,  2.5500e-01, -3.9061e-01,\n",
       "          -2.9545e-01,  1.4460e-01, -9.1103e-02, -2.7994e-01, -6.5774e-02,\n",
       "           8.5205e-02,  1.8328e-01,  2.3383e-01, -2.2909e-01,  4.2486e-01,\n",
       "           7.5080e-02,  4.4448e-01,  1.1976e-01, -2.8988e-01,  5.7364e-02,\n",
       "           1.4637e-01]]], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=512, \n",
    "    d_model=256\n",
    ")\n",
    "\n",
    "att_layer = TransformerAttention(\n",
    "    d_model=256, \n",
    "    num_heads=8\n",
    ")\n",
    "\n",
    "x = enc(torch.tensor([1, 2, 3], device=dev).unsqueeze(0))\n",
    "att_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065acf4",
   "metadata": {},
   "source": [
    "## Feed-Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96025690",
   "metadata": {},
   "source": [
    "- According, to section 3.3 of the paper, this has 2 layers\n",
    "- d_model -> d_ff -> d_model\n",
    "- same parameters for every position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4cdf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 d_ff: int):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_ff,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Linear(\n",
    "            in_features=self.d_ff,\n",
    "            out_features=self.d_model,\n",
    "            device=dev\n",
    "        )\n",
    "        \n",
    "    def forward(self, input:torch.Tensor):\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "        f2 = self.fc2(f1)\n",
    "        return f2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d124573",
   "metadata": {},
   "source": [
    "## The Encoder Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24f19136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int, \n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.att_layer = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=dev)\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_att = self.att_layer(x)\n",
    "\n",
    "        x_att = self.dropout(x_att)\n",
    "        x_norm1 = self.norm1(x + x_att)\n",
    "\n",
    "        x_ff = self.ffn(x_norm1)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm2 = self.norm2(x_ff + x_norm1)\n",
    "        \n",
    "        return x_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c205fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    d_model= 512,\n",
    "    d_ff=2048,\n",
    "    num_heads=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "756e27b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (att_layer): TransformerAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (ffn): FeedForwardNetwork(\n",
       "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "encoder(\n",
    "    enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0))\n",
    "    )\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1538db08",
   "metadata": {},
   "source": [
    "## Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf6438",
   "metadata": {},
   "source": [
    "only the final encoder output is used for all the decoder layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78af3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.att_layer1 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.att_layer2 = TransformerAttention(\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.ffn = FeedForwardNetwork(\n",
    "            d_model=self.d_model,\n",
    "            d_ff = self.d_ff\n",
    "        )\n",
    "\n",
    "        self.norm3 = nn.LayerNorm(self.d_model, device=dev)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=dev), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, value=float('-inf'))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor, \n",
    "                cross_input:torch.Tensor,\n",
    "                padding_mask:torch.Tensor = None\n",
    "                ):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "\n",
    "        causal_mask = self.create_causal_mask(seq_len=seq_length)\n",
    "        causal_mask = causal_mask.unsqueeze(0).unsqueeze(1) #unsqeeze the mast for self attention\n",
    "\n",
    "        x_att1 = self.att_layer1( #self-attention\n",
    "            seq=x,\n",
    "            att_mask = causal_mask\n",
    "        )\n",
    "\n",
    "        x_att1 = self.dropout(x_att1)\n",
    "        x_norm1 = self.norm1(x_att1 + x)\n",
    "\n",
    "        x_att2 = self.att_layer2( #cross attention\n",
    "            seq=x_norm1,\n",
    "            key_value_states=cross_input,\n",
    "            att_mask = padding_mask\n",
    "        )\n",
    "\n",
    "        x_att2 = self.dropout(x_att2)\n",
    "        x_norm2 = self.norm2(x_att2)\n",
    "\n",
    "        x_ff = self.ffn(x_norm2)\n",
    "\n",
    "        x_ff = self.dropout(x_ff)\n",
    "        x_norm3 = self.norm3(x_ff)\n",
    "        \n",
    "        return x_norm3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f58b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1775, -2.3496,  0.2176,  ..., -0.0558,  0.3318,  0.4629],\n",
       "         [-1.0657, -2.5011, -0.0984,  ..., -0.0984, -0.0984, -0.2286],\n",
       "         [-0.9635, -2.5325,  0.1274,  ..., -0.8475,  0.0596,  0.2640]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "decoder = TransformerDecoder(\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "x = encoder(\n",
    "    enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0))\n",
    "    )\n",
    "\n",
    "decoder.forward(enc(torch.tensor([1, 2, 4], device=dev).unsqueeze(0)), x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb7bdf",
   "metadata": {},
   "source": [
    "## Transformer Encoder and Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a12b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 N_enc: int,\n",
    "                 N_dec: int, \n",
    "                 d_model:int,\n",
    "                 num_heads: int, \n",
    "                 d_ff: int,\n",
    "                 dropout_p = 0.1\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        self.encoder_stack = nn.ModuleList([\n",
    "            TransformerEncoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                d_ff=self.d_ff,\n",
    "                dropout_p=self.dropout_p\n",
    "            ) for _ in range(N_enc)\n",
    "        ])\n",
    "\n",
    "        self.decoder_stack = nn.ModuleList([\n",
    "            TransformerDecoder(\n",
    "                d_model=self.d_model,\n",
    "                num_heads=self.num_heads,\n",
    "                d_ff=self.d_ff,\n",
    "                dropout_p=self.dropout_p\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y:torch.Tensor) -> torch.Tensor:\n",
    "        #pass through the encoder stack\n",
    "        encoder_output = x\n",
    "        for encoder in self.encoder_stack:\n",
    "            encoder_output = encoder(encoder_output)\n",
    "\n",
    "        #pass through the decoder stack\n",
    "        #uses only the final encoder input\n",
    "        decoder_output = y\n",
    "        for decoder in self.decoder_stack:\n",
    "            decoder_output = decoder(decoder_output, cross_input=encoder_output)\n",
    "\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6794bcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1387, -0.0440, -1.0679,  ..., -0.0614,  0.7674,  0.8670],\n",
       "         [-1.0070,  1.0224, -0.7265,  ...,  0.3842,  0.6753,  0.3150],\n",
       "         [-1.0564,  1.3978, -1.8241,  ..., -0.2619,  0.8187, -0.0142]]],\n",
       "       device='mps:0', grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = EmbeddingWithPositionalEncoding(\n",
    "    vocab_size=100,\n",
    "    d_embed=1024, \n",
    "    d_model=512\n",
    ")\n",
    "\n",
    "tf = TransformerEncoderDecoder(\n",
    "    N_enc = 6,\n",
    "    N_dec=6,\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048\n",
    ")\n",
    "\n",
    "tf(enc(\n",
    "    torch.tensor([1, 2, 3], device=dev).unsqueeze(0)\n",
    "    ), enc(\n",
    "    torch.tensor([1, 2, 3], device=dev).unsqueeze(0)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac9b49",
   "metadata": {},
   "source": [
    "## Full Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "436b2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 N_enc: int,\n",
    "                 N_dec: int,\n",
    "                 vocab_size:int, \n",
    "                 d_embed: int,\n",
    "                 d_model: int,\n",
    "                 num_heads: int,\n",
    "                 d_ff: int,\n",
    "                 d_tgt_vocab: int,\n",
    "                 dropout_p = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.N_enc = N_enc\n",
    "        self.N_dec = N_dec\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.num_heads = num_heads\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout_p = dropout_p\n",
    "        self.d_tgt_vocab = d_tgt_vocab\n",
    "\n",
    "        self.src_embedder = EmbeddingWithPositionalEncoding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_embed=self.d_embed,\n",
    "            d_model=self.d_model,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.tgt_embedder = EmbeddingWithPositionalEncoding(\n",
    "            vocab_size=self.vocab_size,\n",
    "            d_embed=self.d_embed,\n",
    "            d_model=self.d_model,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.encoder_decoder_stack = TransformerEncoderDecoder(\n",
    "            N_enc=self.N_enc,\n",
    "            N_dec=self.N_dec,\n",
    "            d_model=self.d_model,\n",
    "            num_heads=self.num_heads,\n",
    "            d_ff=self.d_ff,\n",
    "            dropout_p=self.dropout_p\n",
    "        )\n",
    "\n",
    "        self.output_proj = nn.Linear(\n",
    "            in_features=self.d_model,\n",
    "            out_features=self.d_tgt_vocab,\n",
    "            device=dev\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift_target_right(tgt_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len = tgt_tokens.size() # no d_model since, no Embedding done\n",
    "        zer = torch.zeros(\n",
    "            size=(batch_size, 1),\n",
    "            device=dev,\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        return torch.concat([\n",
    "            zer, \n",
    "            tgt_tokens[:, :-1]], \n",
    "            dim=1).to(dev)\n",
    "\n",
    "    def forward(self, \n",
    "                src_tokens:torch.Tensor, \n",
    "                tgt_tokens:torch.Tensor\n",
    "                ) -> torch.Tensor:\n",
    "        \n",
    "        tgt_tokens = self.shift_target_right(tgt_tokens) \n",
    "        # shifting is needed to prevent information leakage. \n",
    "        # it allows parallel traning in spite of hiding the token \n",
    "        # to be predicted.\n",
    "        inp_embed = self.src_embedder(src_tokens)\n",
    "        tgt_embed = self.tgt_embedder(tgt_tokens)\n",
    "\n",
    "        enc_dec_out = self.encoder_decoder_stack.forward(inp_embed, tgt_embed)\n",
    "        \n",
    "        out = self.output_proj(enc_dec_out)\n",
    "        log_probs = self.softmax(out)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98bf88bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = Transformer(\n",
    "    N_enc = 6,\n",
    "    N_dec=6,\n",
    "    d_model=512,\n",
    "    num_heads=4,\n",
    "    d_ff=2048,\n",
    "    vocab_size=100,\n",
    "    d_embed=1024,\n",
    "    d_tgt_vocab=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27b9e5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 24,426,084\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in tf.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5681c4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 100])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf(\n",
    "    src_tokens=torch.tensor([1., 2., 3.], device=dev, dtype=torch.long).unsqueeze(0),\n",
    "    tgt_tokens=torch.tensor([1., 2., 3.], device=dev, dtype=torch.long).unsqueeze(0),\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d671eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
